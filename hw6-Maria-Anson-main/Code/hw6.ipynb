{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>doc</th>\n",
       "      <th>es</th>\n",
       "      <th>bm</th>\n",
       "      <th>okapi</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>lml</th>\n",
       "      <th>lmjm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>AP891027-0016</td>\n",
       "      <td>1.2539542</td>\n",
       "      <td>1.2306580370117726</td>\n",
       "      <td>0.4129732895994717</td>\n",
       "      <td>0.42804370058603747</td>\n",
       "      <td>-310.70553398005205</td>\n",
       "      <td>-305.21017024928557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>AP890216-0271</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>-400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>AP890707-0181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>-400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85</td>\n",
       "      <td>AP891123-0121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>-400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>AP890502-0155</td>\n",
       "      <td>2.5423493</td>\n",
       "      <td>2.499189552514198</td>\n",
       "      <td>0.592028597572928</td>\n",
       "      <td>0.8161166838548195</td>\n",
       "      <td>-222.22092170174858</td>\n",
       "      <td>-211.36201934113882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query            doc         es                  bm               okapi  \\\n",
       "0    85  AP891027-0016  1.2539542  1.2306580370117726  0.4129732895994717   \n",
       "1    85  AP890216-0271          0                 0.0                 0.0   \n",
       "2    85  AP890707-0181          0                 0.0                 0.0   \n",
       "3    85  AP891123-0121          0                 0.0                 0.0   \n",
       "4    85  AP890502-0155  2.5423493   2.499189552514198   0.592028597572928   \n",
       "\n",
       "                 tfidf                  lml                 lmjm  label  \n",
       "0  0.42804370058603747  -310.70553398005205  -305.21017024928557      0  \n",
       "1                  0.0                 -400                 -400      0  \n",
       "2                  0.0                 -400                 -400      0  \n",
       "3                  0.0                 -400                 -400      0  \n",
       "4   0.8161166838548195  -222.22092170174858  -211.36201934113882      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataFrameCreator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.qrel_docs = {}\n",
    "        self.query_doc = {}\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        self.score_files = {\n",
    "            \"es\": \"scores/results_ES.txt\",\n",
    "            \"bm\": \"scores/results_BM25.txt\",\n",
    "            \"okapi\": \"scores/results_OKAPI.txt\",\n",
    "            \"tfidf\": \"scores/results_TFIDF.txt\",\n",
    "            \"lml\": \"scores/results_LMLAPLACE.txt\",\n",
    "            \"lmjm\": \"scores/results_LMJM.txt\"\n",
    "        }\n",
    "\n",
    "        self.read_qrel()\n",
    "        self.read_score_files()\n",
    "        self.get_final_query_doc()\n",
    "        self.get_data_frame()\n",
    "\n",
    "    def read_qrel(self):\n",
    "        with open(\"qrels.adhoc.51-100.AP89.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                query_id, _, doc_id, rel = line.split()\n",
    "                self.qrel_docs.setdefault(query_id, {})[doc_id] = rel\n",
    "\n",
    "    def read_score_files(self):\n",
    "        for method, file_path in self.score_files.items():\n",
    "            scores = {}\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    query_id, _, doc_id, _, score, _ = line.split()\n",
    "                    scores.setdefault(query_id, {})[doc_id] = score\n",
    "            setattr(self, f\"{method}_score\", scores)\n",
    "\n",
    "    def get_final_query_doc(self):\n",
    "        for method in self.score_files.keys():\n",
    "            for query_id, scores in getattr(self, f\"{method}_score\").items():\n",
    "                self.query_doc.setdefault(query_id, set()).update(scores.keys())\n",
    "\n",
    "    def get_data_frame(self):\n",
    "        data = []\n",
    "        for query_id, docs in self.query_doc.items():\n",
    "            for doc_id in docs:\n",
    "                row = {\n",
    "                    \"query\": query_id,\n",
    "                    \"doc\": doc_id,\n",
    "                    **{method: getattr(self, f\"{method}_score\")[query_id].get(doc_id, 0) for method in self.score_files.keys()},\n",
    "                    \"label\": self.qrel_docs.get(query_id, {}).get(doc_id, 0)\n",
    "                }\n",
    "                data.append(row)\n",
    "        self.df = pd.DataFrame(data)\n",
    "\n",
    "dfc = DataFrameCreator()\n",
    "df = dfc.df\n",
    "df['label'] = df['label'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train queries: ['58' '95' '85' '63' '87' '77' '100' '59' '80' '62' '56' '94' '61' '71'\n",
      " '64' '98' '68' '60' '57' '91']\n",
      "Test queries: ['99' '54' '89' '97' '93']\n"
     ]
    }
   ],
   "source": [
    "# randomly split into 20 and 5\n",
    "queries = df['query'].unique()\n",
    "np.random.seed(42)\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "train_queries = queries[:20]\n",
    "test_queries = queries[20:]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train queries: {train_queries}\")\n",
    "print(f\"Test queries: {test_queries}\")\n",
    "\n",
    "train_df = df[df['query'].isin(train_queries)]\n",
    "test_df = df[df['query'].isin(test_queries)]\n",
    "\n",
    "train_features = train_df.drop(columns=['query', 'doc', 'label'])\n",
    "train_labels = train_df['label']\n",
    "\n",
    "test_features = test_df.drop(columns=['query', 'doc', 'label'])\n",
    "test_labels = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_score(file_path, df):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for idx, row in df.iterrows():\n",
    "            line = \"{} Q0 {} 1 {} Exp\\n\".format(row[\"query\"], row[\"doc\"], row[\"predicted_label\"])\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1688274\n",
      "           1       0.50      0.12      0.19      1566\n",
      "\n",
      "    accuracy                           1.00   1689840\n",
      "   macro avg       0.75      0.56      0.60   1689840\n",
      "weighted avg       1.00      1.00      1.00   1689840\n",
      "\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    422195\n",
      "           1       0.45      0.34      0.39       265\n",
      "\n",
      "    accuracy                           1.00    422460\n",
      "   macro avg       0.73      0.67      0.69    422460\n",
      "weighted avg       1.00      1.00      1.00    422460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a classification and get ranked output based on probability\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_features, train_labels)\n",
    "\n",
    "train_df['predicted_label'] = lr.predict(train_features)\n",
    "test_df['predicted_label'] = lr.predict(test_features)\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Train Classification Report\")\n",
    "print(classification_report(train_labels, train_df['predicted_label']))\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(test_labels, test_df['predicted_label']))\n",
    "\n",
    "# Sort based on probability\n",
    "train_df['predicted_label'] = lr.predict_proba(train_features)[:,1]\n",
    "test_df['predicted_label'] = lr.predict_proba(test_features)[:,1]\n",
    "\n",
    "train_df = train_df.sort_values(by=['query', 'predicted_label'], ascending=[True, False])\n",
    "test_df = test_df.sort_values(by=['query', 'predicted_label'], ascending=[True, False])\n",
    "\n",
    "# Write the output to file\n",
    "write_score(\"outputs/train_scores_lr.txt\", train_df)\n",
    "write_score(\"outputs/test_scores_lr.txt\", test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 20\n",
      "\n",
      "Queryid (Num):       20\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    20000\n",
      "    Relevant:      1566\n",
      "    Rel_ret:       1050\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.7063\n",
      "    at 0.10       0.5176\n",
      "    at 0.20       0.4616\n",
      "    at 0.30       0.3450\n",
      "    at 0.40       0.2953\n",
      "    at 0.50       0.2331\n",
      "    at 0.60       0.1928\n",
      "    at 0.70       0.1703\n",
      "    at 0.80       0.1306\n",
      "    at 0.90       0.0542\n",
      "    at 1.00       0.0228\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2683\n",
      "Precision:\n",
      "  At    5 docs:   0.4400\n",
      "  At   10 docs:   0.4050\n",
      "  At   15 docs:   0.3867\n",
      "  At   20 docs:   0.3650\n",
      "  At   30 docs:   0.3317\n",
      "  At  100 docs:   0.2175\n",
      "  At  200 docs:   0.1592\n",
      "  At  500 docs:   0.0890\n",
      "  At 1000 docs:   0.0525\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2930\n",
      "Error due to 5\n",
      "\n",
      "Queryid (Num):        5\n",
      "Total number of documents over all queries\n",
      "    Retrieved:     5000\n",
      "    Relevant:       266\n",
      "    Rel_ret:        250\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.9111\n",
      "    at 0.10       0.6402\n",
      "    at 0.20       0.5800\n",
      "    at 0.30       0.5172\n",
      "    at 0.40       0.5007\n",
      "    at 0.50       0.4650\n",
      "    at 0.60       0.3929\n",
      "    at 0.70       0.3524\n",
      "    at 0.80       0.2801\n",
      "    at 0.90       0.1062\n",
      "    at 1.00       0.0206\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.4059\n",
      "Precision:\n",
      "  At    5 docs:   0.5600\n",
      "  At   10 docs:   0.5000\n",
      "  At   15 docs:   0.4667\n",
      "  At   20 docs:   0.4600\n",
      "  At   30 docs:   0.4400\n",
      "  At  100 docs:   0.3260\n",
      "  At  200 docs:   0.2070\n",
      "  At  500 docs:   0.0964\n",
      "  At 1000 docs:   0.0500\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.4442\n"
     ]
    }
   ],
   "source": [
    "!perl trec_eval.pl qrels.adhoc.51-100.AP89.txt outputs/train_scores_lr.txt\n",
    "!perl trec_eval.pl qrels.adhoc.51-100.AP89.txt outputs/test_scores_lr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1688274\n",
      "           1       0.99      0.81      0.89      1566\n",
      "\n",
      "    accuracy                           1.00   1689840\n",
      "   macro avg       1.00      0.90      0.94   1689840\n",
      "weighted avg       1.00      1.00      1.00   1689840\n",
      "\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    422195\n",
      "           1       0.38      0.08      0.13       265\n",
      "\n",
      "    accuracy                           1.00    422460\n",
      "   macro avg       0.69      0.54      0.57    422460\n",
      "weighted avg       1.00      1.00      1.00    422460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest and get ranked output based on probability\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "train_df['predicted_label'] = rf.predict(train_features)\n",
    "test_df['predicted_label'] = rf.predict(test_features)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Train Classification Report\")\n",
    "print(classification_report(train_labels, train_df['predicted_label']))\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(test_labels, test_df['predicted_label']))\n",
    "\n",
    "# Sort based on probability\n",
    "train_df['predicted_label'] = rf.predict_proba(train_features)[:,1]\n",
    "test_df['predicted_label'] = rf.predict_proba(test_features)[:,1]\n",
    "\n",
    "train_df = train_df.sort_values(by=['query', 'predicted_label'], ascending=[True, False])\n",
    "test_df = test_df.sort_values(by=['query', 'predicted_label'], ascending=[True, False])\n",
    "\n",
    "# Write the output to file\n",
    "write_score(\"outputs/train_scores_rf.txt\", train_df)\n",
    "write_score(\"outputs/test_scores_rf.txt\", test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 20\n",
      "\n",
      "Queryid (Num):       20\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    20000\n",
      "    Relevant:      1566\n",
      "    Rel_ret:         17\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.0020\n",
      "    at 0.10       0.0000\n",
      "    at 0.20       0.0000\n",
      "    at 0.30       0.0000\n",
      "    at 0.40       0.0000\n",
      "    at 0.50       0.0000\n",
      "    at 0.60       0.0000\n",
      "    at 0.70       0.0000\n",
      "    at 0.80       0.0000\n",
      "    at 0.90       0.0000\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.0000\n",
      "Precision:\n",
      "  At    5 docs:   0.0000\n",
      "  At   10 docs:   0.0000\n",
      "  At   15 docs:   0.0000\n",
      "  At   20 docs:   0.0000\n",
      "  At   30 docs:   0.0000\n",
      "  At  100 docs:   0.0005\n",
      "  At  200 docs:   0.0003\n",
      "  At  500 docs:   0.0005\n",
      "  At 1000 docs:   0.0009\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.0004\n",
      "Error due to 5\n",
      "\n",
      "Queryid (Num):        5\n",
      "Total number of documents over all queries\n",
      "    Retrieved:     5000\n",
      "    Relevant:       266\n",
      "    Rel_ret:          2\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.0019\n",
      "    at 0.10       0.0000\n",
      "    at 0.20       0.0000\n",
      "    at 0.30       0.0000\n",
      "    at 0.40       0.0000\n",
      "    at 0.50       0.0000\n",
      "    at 0.60       0.0000\n",
      "    at 0.70       0.0000\n",
      "    at 0.80       0.0000\n",
      "    at 0.90       0.0000\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.0000\n",
      "Precision:\n",
      "  At    5 docs:   0.0000\n",
      "  At   10 docs:   0.0000\n",
      "  At   15 docs:   0.0000\n",
      "  At   20 docs:   0.0000\n",
      "  At   30 docs:   0.0000\n",
      "  At  100 docs:   0.0000\n",
      "  At  200 docs:   0.0010\n",
      "  At  500 docs:   0.0004\n",
      "  At 1000 docs:   0.0004\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.0000\n"
     ]
    }
   ],
   "source": [
    "!perl trec_eval.pl qrels.adhoc.51-100.AP89.txt outputs/train_scores_rf.txt\n",
    "!perl trec_eval.pl qrels.adhoc.51-100.AP89.txt outputs/test_scores_rf.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
