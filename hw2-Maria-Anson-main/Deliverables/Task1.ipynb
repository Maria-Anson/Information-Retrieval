{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own index that would replace the one from elasticsearch used in HW1, then index the document collection used in HW1. Your index should be able to handle large numbers of documents and terms without using excessive memory or disk I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import zlib\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Path to the directory containing files\n",
    "DIRECTORY_PATH = '../ap89_collection/'\n",
    "QUERY_PATH = '../AP_DATA/query_desc.51-100.short.txt'\n",
    "INDEX_NAME = 'ap89_collection_hw2'\n",
    "STOP_WORDS_PATH = '../AP_DATA/stoplist.txt'\n",
    "BATCH_SIZE = 1000\n",
    "STEMMED_PATH = 'stemmed/'\n",
    "UNSTEMMED_PATH = 'unstemmed/'\n",
    "\n",
    "STEMMED_CATALOG_PATH = os.path.join(STEMMED_PATH, 'catalog/')\n",
    "UNSTEMMED_CATALOG_PATH = os.path.join(UNSTEMMED_PATH, 'catalog/')\n",
    "STEMMED_INDEX_PATH = os.path.join(STEMMED_PATH, 'index/')\n",
    "UNSTEMMED_INDEX_PATH = os.path.join(UNSTEMMED_PATH, 'index/')\n",
    "STEMMED_MERGE_PATH = os.path.join(STEMMED_PATH, 'merge/')\n",
    "UNSTEMMED_MERGE_PATH = os.path.join(UNSTEMMED_PATH, 'merge/')\n",
    "STEMMED_RESULTS_PATH = os.path.join(STEMMED_PATH, 'results/')\n",
    "UNSTEMMED_RESULTS_PATH = os.path.join(UNSTEMMED_PATH, 'results/')\n",
    "STEMMED_COMPRESSED_PATH = os.path.join(STEMMED_PATH, 'compressed/')\n",
    "UNSTEMMED_COMPRESSED_PATH = os.path.join(UNSTEMMED_PATH, 'compressed/')\n",
    "STEMMED_DECOMPRESSED_PATH = os.path.join(STEMMED_PATH, 'decompressed/')\n",
    "UNSTEMMED_DECOMPRESSED_PATH = os.path.join(UNSTEMMED_PATH, 'decompressed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - A tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa528d606c2a42e4b88ad1321b2e5324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8468), Label(value='0 / 8468'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3435cc9d1a2e481aaf638f56e260b3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8468), Label(value='0 / 8468'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCNO</th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FIRST</th>\n",
       "      <th>SECOND</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>BYLINE</th>\n",
       "      <th>DATELINE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>STEMMED_TEXT</th>\n",
       "      <th>UNSTEMMED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP891220-0001</td>\n",
       "      <td>AP-NR-12-20-89 2343EST</td>\n",
       "      <td>u i BC-Panama-Resistance     12-20 0723</td>\n",
       "      <td>BC-Panama-Resistance,0746</td>\n",
       "      <td>[Defense Forces, Noriega Resist; Loyalty Remai...</td>\n",
       "      <td>By ELOY O. AGUILAR</td>\n",
       "      <td>PANAMA CITY, Panama (AP)</td>\n",
       "      <td>Instead of collapsing when the United States t...</td>\n",
       "      <td>collaps unit state threw militari gen. manuel ...</td>\n",
       "      <td>collapsing united states threw military gen. m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP891220-0002</td>\n",
       "      <td>AP-NR-12-20-89 0031EST</td>\n",
       "      <td>r a AM-RollingStones     12-20 0372</td>\n",
       "      <td>AM-Rolling Stones,0385</td>\n",
       "      <td>[Stones Pay Per View Concert Tops North Americ...</td>\n",
       "      <td>By HENRY STERN</td>\n",
       "      <td>ATLANTIC CITY, N.J. (AP)</td>\n",
       "      <td>The Rolling Stones capped the next-to-last sho...</td>\n",
       "      <td>roll stone cap next-to-last show 32-citi north...</td>\n",
       "      <td>rolling stones capped next-to-last show 32-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP891220-0003</td>\n",
       "      <td>AP-NR-12-20-89 0038EST</td>\n",
       "      <td>r a PM-Christmas-Jesus'KinI     12-20 1240</td>\n",
       "      <td>PM-Christmas-Jesus' Kin I,1277</td>\n",
       "      <td>[Part I: Jesus Descended From Checkered Ancest...</td>\n",
       "      <td>By GEORGE W. CORNELL</td>\n",
       "      <td>None</td>\n",
       "      <td>This first installment of a three-part Christm...</td>\n",
       "      <td>instal three-part christma seri rel jesu deal ...</td>\n",
       "      <td>installment three-part christmas series relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP891220-0004</td>\n",
       "      <td>AP-NR-12-20-89 0039EST</td>\n",
       "      <td>r i PM-SAfrica-CoupTrial     12-20 0176</td>\n",
       "      <td>PM-SAfrica-Coup Trial,0178</td>\n",
       "      <td>[Leader Of Failed Homeland Coup Gets 18-Year P...</td>\n",
       "      <td>None</td>\n",
       "      <td>JOHANNESBURG, South Africa (AP)</td>\n",
       "      <td>A soldier who led a failed coup attempt in one...</td>\n",
       "      <td>soldier led fail coup attempt south africa 's ...</td>\n",
       "      <td>soldier led failed coup attempt south africa '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP891220-0005</td>\n",
       "      <td>AP-NR-12-20-89 0039EST</td>\n",
       "      <td>r a PM-RobinsonProfile     12-20 0558</td>\n",
       "      <td>PM-Robinson Profile,0576</td>\n",
       "      <td>[Savannah Leaders Praise Slain Alderman, Eds: ...</td>\n",
       "      <td>By LAURAN NEERGAARD</td>\n",
       "      <td>SAVANNAH, Ga. (AP)</td>\n",
       "      <td>Robert Robinson, the alderman and lawyer assas...</td>\n",
       "      <td>robert robinson alderman lawyer assassin mail ...</td>\n",
       "      <td>robert robinson alderman lawyer assassinated m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DOCNO                  FILEID  \\\n",
       "0  AP891220-0001  AP-NR-12-20-89 2343EST   \n",
       "1  AP891220-0002  AP-NR-12-20-89 0031EST   \n",
       "2  AP891220-0003  AP-NR-12-20-89 0038EST   \n",
       "3  AP891220-0004  AP-NR-12-20-89 0039EST   \n",
       "4  AP891220-0005  AP-NR-12-20-89 0039EST   \n",
       "\n",
       "                                        FIRST                          SECOND  \\\n",
       "0     u i BC-Panama-Resistance     12-20 0723       BC-Panama-Resistance,0746   \n",
       "1         r a AM-RollingStones     12-20 0372          AM-Rolling Stones,0385   \n",
       "2  r a PM-Christmas-Jesus'KinI     12-20 1240  PM-Christmas-Jesus' Kin I,1277   \n",
       "3     r i PM-SAfrica-CoupTrial     12-20 0176      PM-SAfrica-Coup Trial,0178   \n",
       "4       r a PM-RobinsonProfile     12-20 0558        PM-Robinson Profile,0576   \n",
       "\n",
       "                                                HEAD                BYLINE  \\\n",
       "0  [Defense Forces, Noriega Resist; Loyalty Remai...    By ELOY O. AGUILAR   \n",
       "1  [Stones Pay Per View Concert Tops North Americ...        By HENRY STERN   \n",
       "2  [Part I: Jesus Descended From Checkered Ancest...  By GEORGE W. CORNELL   \n",
       "3  [Leader Of Failed Homeland Coup Gets 18-Year P...                  None   \n",
       "4  [Savannah Leaders Praise Slain Alderman, Eds: ...   By LAURAN NEERGAARD   \n",
       "\n",
       "                          DATELINE  \\\n",
       "0         PANAMA CITY, Panama (AP)   \n",
       "1         ATLANTIC CITY, N.J. (AP)   \n",
       "2                             None   \n",
       "3  JOHANNESBURG, South Africa (AP)   \n",
       "4               SAVANNAH, Ga. (AP)   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  Instead of collapsing when the United States t...   \n",
       "1  The Rolling Stones capped the next-to-last sho...   \n",
       "2  This first installment of a three-part Christm...   \n",
       "3  A soldier who led a failed coup attempt in one...   \n",
       "4  Robert Robinson, the alderman and lawyer assas...   \n",
       "\n",
       "                                        STEMMED_TEXT  \\\n",
       "0  collaps unit state threw militari gen. manuel ...   \n",
       "1  roll stone cap next-to-last show 32-citi north...   \n",
       "2  instal three-part christma seri rel jesu deal ...   \n",
       "3  soldier led fail coup attempt south africa 's ...   \n",
       "4  robert robinson alderman lawyer assassin mail ...   \n",
       "\n",
       "                                      UNSTEMMED_TEXT  \n",
       "0  collapsing united states threw military gen. m...  \n",
       "1  rolling stones capped next-to-last show 32-cit...  \n",
       "2  installment three-part christmas series relati...  \n",
       "3  soldier led failed coup attempt south africa '...  \n",
       "4  robert robinson alderman lawyer assassinated m...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_documents(path=DIRECTORY_PATH, stop_words_path=STOP_WORDS_PATH):\n",
    "    data = []\n",
    "    \n",
    "    stop_words = set()\n",
    "    with open(stop_words_path, 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Use regular expression to extract each document\n",
    "        docs = re.findall(r'<DOC>.*?</DOC>', content, flags=re.DOTALL)\n",
    "\n",
    "        # Extract metadata and text for each document\n",
    "        for doc in docs:\n",
    "            docno_match = re.search(r'<DOCNO>(.*?)</DOCNO>', doc)\n",
    "            fileid_match = re.search(r'<FILEID>(.*?)</FILEID>', doc)\n",
    "            first_match = re.search(r'<FIRST>(.*?)</FIRST>', doc)\n",
    "            second_match = re.search(r'<SECOND>(.*?)</SECOND>', doc)\n",
    "            head_matches = re.findall(r'<HEAD>(.*?)</HEAD>', doc)\n",
    "            byline_match = re.search(r'<BYLINE>(.*?)</BYLINE>', doc)\n",
    "            dateline_match = re.search(r'<DATELINE>(.*?)</DATELINE>', doc)\n",
    "            text_match = re.search(r'<TEXT>(.*?)</TEXT>', doc, flags=re.DOTALL)\n",
    "\n",
    "            # Create a dictionary for each document\n",
    "            doc_data = {\n",
    "                'DOCNO': docno_match.group(1).strip() if docno_match else None,\n",
    "                'FILEID': fileid_match.group(1).strip() if fileid_match else None,\n",
    "                'FIRST': first_match.group(1).strip() if first_match else None,\n",
    "                'SECOND': second_match.group(1).strip() if second_match else None,\n",
    "                'HEAD': head_matches if head_matches else None,\n",
    "                'BYLINE': byline_match.group(1).strip() if byline_match else None,\n",
    "                'DATELINE': dateline_match.group(1).strip() if dateline_match else None,\n",
    "                'TEXT': text_match.group(1).strip() if text_match else None\n",
    "            }\n",
    "            data.append(doc_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['TEXT'] = df['TEXT'].replace(r'\\n', ' ', regex=True)\n",
    "    df['STEMMED_TEXT'] = df['TEXT'].parallel_apply(lambda x: preprocess_text(x, stop_words, stem=True))\n",
    "    df['UNSTEMMED_TEXT'] = df['TEXT'].parallel_apply(lambda x: preprocess_text(x, stop_words, stem=False))\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_text(text, stop_words, stem=True):\n",
    "    # Tokenize the words and then remove the stop words\n",
    "    # then use porter stemmer to stem the words\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuations = set(string.punctuation)\n",
    "    # special_exclusions = ['``', \"''\", '..', '...', '....', '--']\n",
    "    # punctuations.update(special_exclusions)\n",
    "    # Remove words from stop words and punctuation\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuations]\n",
    "    # Remove if same puncatuations are repeated\n",
    "    for word in tokens:\n",
    "        if all(c in string.punctuation for c in word):\n",
    "            tokens.remove(word)\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df = parse_documents()\n",
    "df.to_csv('ap89_processed_collection.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stemmed Index\n",
      "Processing batch 0/ 84\n",
      "Processing batch 1/ 84\n",
      "Processing batch 2/ 84\n",
      "Processing batch 3/ 84\n",
      "Processing batch 4/ 84\n",
      "Processing batch 5/ 84\n",
      "Processing batch 6/ 84\n",
      "Processing batch 7/ 84\n",
      "Processing batch 8/ 84\n",
      "Processing batch 9/ 84\n",
      "Processing batch 10/ 84\n",
      "Processing batch 11/ 84\n",
      "Processing batch 12/ 84\n",
      "Processing batch 13/ 84\n",
      "Processing batch 14/ 84\n",
      "Processing batch 15/ 84\n",
      "Processing batch 16/ 84\n",
      "Processing batch 17/ 84\n",
      "Processing batch 18/ 84\n",
      "Processing batch 19/ 84\n",
      "Processing batch 20/ 84\n",
      "Processing batch 21/ 84\n",
      "Processing batch 22/ 84\n",
      "Processing batch 23/ 84\n",
      "Processing batch 24/ 84\n",
      "Processing batch 25/ 84\n",
      "Processing batch 26/ 84\n",
      "Processing batch 27/ 84\n",
      "Processing batch 28/ 84\n",
      "Processing batch 29/ 84\n",
      "Processing batch 30/ 84\n",
      "Processing batch 31/ 84\n",
      "Processing batch 32/ 84\n",
      "Processing batch 33/ 84\n",
      "Processing batch 34/ 84\n",
      "Processing batch 35/ 84\n",
      "Processing batch 36/ 84\n",
      "Processing batch 37/ 84\n",
      "Processing batch 38/ 84\n",
      "Processing batch 39/ 84\n",
      "Processing batch 40/ 84\n",
      "Processing batch 41/ 84\n",
      "Processing batch 42/ 84\n",
      "Processing batch 43/ 84\n",
      "Processing batch 44/ 84\n",
      "Processing batch 45/ 84\n",
      "Processing batch 46/ 84\n",
      "Processing batch 47/ 84\n",
      "Processing batch 48/ 84\n",
      "Processing batch 49/ 84\n",
      "Processing batch 50/ 84\n",
      "Processing batch 51/ 84\n",
      "Processing batch 52/ 84\n",
      "Processing batch 53/ 84\n",
      "Processing batch 54/ 84\n",
      "Processing batch 55/ 84\n",
      "Processing batch 56/ 84\n",
      "Processing batch 57/ 84\n",
      "Processing batch 58/ 84\n",
      "Processing batch 59/ 84\n",
      "Processing batch 60/ 84\n",
      "Processing batch 61/ 84\n",
      "Processing batch 62/ 84\n",
      "Processing batch 63/ 84\n",
      "Processing batch 64/ 84\n",
      "Processing batch 65/ 84\n",
      "Processing batch 66/ 84\n",
      "Processing batch 67/ 84\n",
      "Processing batch 68/ 84\n",
      "Processing batch 69/ 84\n",
      "Processing batch 70/ 84\n",
      "Processing batch 71/ 84\n",
      "Processing batch 72/ 84\n",
      "Processing batch 73/ 84\n",
      "Processing batch 74/ 84\n",
      "Processing batch 75/ 84\n",
      "Processing batch 76/ 84\n",
      "Processing batch 77/ 84\n",
      "Processing batch 78/ 84\n",
      "Processing batch 79/ 84\n",
      "Processing batch 80/ 84\n",
      "Processing batch 81/ 84\n",
      "Processing batch 82/ 84\n",
      "Processing batch 83/ 84\n",
      "Processing batch 84/ 84\n",
      "Processing Unstemmed Index\n",
      "Processing batch 0/ 84\n",
      "Processing batch 1/ 84\n",
      "Processing batch 2/ 84\n",
      "Processing batch 3/ 84\n",
      "Processing batch 4/ 84\n",
      "Processing batch 5/ 84\n",
      "Processing batch 6/ 84\n",
      "Processing batch 7/ 84\n",
      "Processing batch 8/ 84\n",
      "Processing batch 9/ 84\n",
      "Processing batch 10/ 84\n",
      "Processing batch 11/ 84\n",
      "Processing batch 12/ 84\n",
      "Processing batch 13/ 84\n",
      "Processing batch 14/ 84\n",
      "Processing batch 15/ 84\n",
      "Processing batch 16/ 84\n",
      "Processing batch 17/ 84\n",
      "Processing batch 18/ 84\n",
      "Processing batch 19/ 84\n",
      "Processing batch 20/ 84\n",
      "Processing batch 21/ 84\n",
      "Processing batch 22/ 84\n",
      "Processing batch 23/ 84\n",
      "Processing batch 24/ 84\n",
      "Processing batch 25/ 84\n",
      "Processing batch 26/ 84\n",
      "Processing batch 27/ 84\n",
      "Processing batch 28/ 84\n",
      "Processing batch 29/ 84\n",
      "Processing batch 30/ 84\n",
      "Processing batch 31/ 84\n",
      "Processing batch 32/ 84\n",
      "Processing batch 33/ 84\n",
      "Processing batch 34/ 84\n",
      "Processing batch 35/ 84\n",
      "Processing batch 36/ 84\n",
      "Processing batch 37/ 84\n",
      "Processing batch 38/ 84\n",
      "Processing batch 39/ 84\n",
      "Processing batch 40/ 84\n",
      "Processing batch 41/ 84\n",
      "Processing batch 42/ 84\n",
      "Processing batch 43/ 84\n",
      "Processing batch 44/ 84\n",
      "Processing batch 45/ 84\n",
      "Processing batch 46/ 84\n",
      "Processing batch 47/ 84\n",
      "Processing batch 48/ 84\n",
      "Processing batch 49/ 84\n",
      "Processing batch 50/ 84\n",
      "Processing batch 51/ 84\n",
      "Processing batch 52/ 84\n",
      "Processing batch 53/ 84\n",
      "Processing batch 54/ 84\n",
      "Processing batch 55/ 84\n",
      "Processing batch 56/ 84\n",
      "Processing batch 57/ 84\n",
      "Processing batch 58/ 84\n",
      "Processing batch 59/ 84\n",
      "Processing batch 60/ 84\n",
      "Processing batch 61/ 84\n",
      "Processing batch 62/ 84\n",
      "Processing batch 63/ 84\n",
      "Processing batch 64/ 84\n",
      "Processing batch 65/ 84\n",
      "Processing batch 66/ 84\n",
      "Processing batch 67/ 84\n",
      "Processing batch 68/ 84\n",
      "Processing batch 69/ 84\n",
      "Processing batch 70/ 84\n",
      "Processing batch 71/ 84\n",
      "Processing batch 72/ 84\n",
      "Processing batch 73/ 84\n",
      "Processing batch 74/ 84\n",
      "Processing batch 75/ 84\n",
      "Processing batch 76/ 84\n",
      "Processing batch 77/ 84\n",
      "Processing batch 78/ 84\n",
      "Processing batch 79/ 84\n",
      "Processing batch 80/ 84\n",
      "Processing batch 81/ 84\n",
      "Processing batch 82/ 84\n",
      "Processing batch 83/ 84\n",
      "Processing batch 84/ 84\n"
     ]
    }
   ],
   "source": [
    "def indexer(subset_df, tokens_dict, stem=True):\n",
    "    doc_nos = subset_df.index\n",
    "    if stem:\n",
    "        tokenized_texts = subset_df['STEMMED_TEXT'].apply(lambda x: x.split())\n",
    "    else:\n",
    "        tokenized_texts = subset_df['UNSTEMMED_TEXT'].apply(lambda x: x.split())\n",
    "    \n",
    "    for doc_no, tokenized_text in zip(doc_nos, tokenized_texts):\n",
    "        for i,token in enumerate(tokenized_text, start=1):\n",
    "            if stem:\n",
    "                token = STEMMED_TOKEN_TO_WORD[token]\n",
    "            else:\n",
    "                token = UNSTEMMED_TOKEN_TO_WORD[token]\n",
    "            if token not in tokens_dict:\n",
    "                tokens_dict[token] = {}\n",
    "            if doc_no not in tokens_dict[token]:\n",
    "                tokens_dict[token][doc_no] = []\n",
    "            tokens_dict[token][doc_no].append(i)\n",
    "\n",
    "def write_index_to_file(tokens_dict, batch_number, stem=True):\n",
    "    if stem:\n",
    "        catalogs_path = STEMMED_CATALOG_PATH\n",
    "        indices_path = STEMMED_INDEX_PATH\n",
    "    else:\n",
    "        catalogs_path = UNSTEMMED_CATALOG_PATH\n",
    "        indices_path = UNSTEMMED_INDEX_PATH\n",
    "    catalog_name = f'{catalogs_path}catalog_{batch_number}.txt'\n",
    "    inverted_index_name = f'{indices_path}inverted_index_{batch_number}.txt'\n",
    "    with open(catalog_name, 'w') as catalog_file , open(inverted_index_name, 'w') as inverted_index:\n",
    "\n",
    "        # Catalog should contain term, offset, and length\n",
    "        offset = 0\n",
    "        for token, docs in tokens_dict.items():\n",
    "            index_string = []\n",
    "            for doc, pos_list in docs.items():\n",
    "                pos_string = \",\".join(str(pos).strip() for pos in pos_list)\n",
    "                index_string.append(f\"{doc}:{pos_string}|\")\n",
    "            index_string = \"\".join(index_string)\n",
    "            inverted_index.write(index_string)\n",
    "            catalog_file.write(f\"{token}:{offset},{len(index_string)}\\n\")\n",
    "            offset += len(index_string)            \n",
    "        \n",
    "def process_batches(dataframe, batch_size, stem = True):\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        print(f'Processing batch {(i+1)//batch_size}/ {len(dataframe)//batch_size}') \n",
    "        tokens_dict = {}\n",
    "        batch = dataframe[i:i + batch_size]\n",
    "        indexer(batch, tokens_dict, stem)\n",
    "        tokens_dict = sort_tokens(tokens_dict)\n",
    "        write_index_to_file(tokens_dict, i, stem)\n",
    "\n",
    "def sort_tokens(tokens):\n",
    "    return OrderedDict(sorted(tokens.items(), key=lambda t: t[0]))\n",
    "\n",
    "# Map doc number to df index\n",
    "DOC_NO_MAP = {i: doc_no for i, doc_no in enumerate(df['DOCNO'])}\n",
    "# Create doc len map for both stemmed and unstemmed\n",
    "DOC_LEN_MAP_STEM = {i: len(doc.split()) for i, doc in enumerate(df['STEMMED_TEXT'])}\n",
    "DOC_LEN_MAP_UNSTEM = {i: len(doc.split()) for i, doc in enumerate(df['UNSTEMMED_TEXT'])}\n",
    "\n",
    "STEMMED_TOKENS = set()\n",
    "UNSTEMMED_TOKENS = set()\n",
    "for doc in df['STEMMED_TEXT']:\n",
    "    STEMMED_TOKENS.update(doc.split())\n",
    "for doc in df['UNSTEMMED_TEXT']:\n",
    "    UNSTEMMED_TOKENS.update(doc.split())\n",
    "# create a map of word to token\n",
    "STEMMED_TOKEN_TO_WORD = {word: i  for i, word in enumerate(STEMMED_TOKENS)}\n",
    "UNSTEMMED_TOKEN_TO_WORD = {word: i  for i, word in enumerate(UNSTEMMED_TOKENS)}\n",
    "\n",
    "print(\"Processing Stemmed Index\")\n",
    "process_batches(df, BATCH_SIZE, stem=True)\n",
    "print(\"Processing Unstemmed Index\")\n",
    "process_batches(df, BATCH_SIZE, stem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask - Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    \"\"\"List files in a directory.\"\"\"\n",
    "    return [os.path.join(directory, filename) for filename in os.listdir(directory)]\n",
    "\n",
    "def load_catalog_from_file(filename):\n",
    "    \"\"\"Load catalog from a file.\"\"\"\n",
    "    catalog = OrderedDict()\n",
    "    with open(filename, encoding=\"ISO-8859-1\", errors='ignore') as file:\n",
    "        for line in file:\n",
    "            token, info = line.rsplit(\":\", 1)\n",
    "            start, length = info.split(\",\")\n",
    "            catalog[int(token.strip())] = (start.strip(), length.strip())\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging stemmed indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [05:23<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied merged files to compressed folder\n",
      "Merging process completed.\n",
      "Merging unstemmed indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:41<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied merged files to compressed folder\n",
      "Merging process completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_indices_and_catalog_without_compression(merge_path, merge_number, catalog1, index1_path, catalog2, index2_path):\n",
    "    \"\"\"Merge indices and catalog.\"\"\"\n",
    "    merged_catalog_path = f\"{merge_path}merged{merge_number}_catalog.txt\"\n",
    "    merged_index_path = f\"{merge_path}merged{merge_number}_index.txt\"\n",
    "    \n",
    "    with open(merged_catalog_path, \"x\") as merged_catalog_file, open(merged_index_path, \"x\") as merged_index_file:\n",
    "        i, j = 0, 0\n",
    "        offset = 0\n",
    "        while i < len(catalog1) and j < len(catalog2):\n",
    "            term1, (start1, length1) = catalog1[i]\n",
    "            term2, (start2, length2) = catalog2[j]\n",
    "\n",
    "            if term1 == term2:\n",
    "                with open(index1_path, \"r\") as index_file1, open(index2_path, \"r\") as index_file2:\n",
    "                    index_file1.seek(int(start1))\n",
    "                    index_file2.seek(int(start2))\n",
    "                    info1 = index_file1.read(int(length1))\n",
    "                    info2 = index_file2.read(int(length2))\n",
    "                new_info = info1 + info2\n",
    "                merged_catalog_file.write(f\"{term1}:{offset},{len(new_info)}\\n\")\n",
    "                merged_index_file.write(new_info)\n",
    "\n",
    "                i += 1\n",
    "                j += 1\n",
    "                offset += len(new_info)\n",
    "            elif term1 > term2:\n",
    "                with open(index2_path, \"r\") as index_file2:\n",
    "                    index_file2.seek(int(start2))\n",
    "                    info2 = index_file2.read(int(length2))\n",
    "                merged_catalog_file.write(f\"{term2}:{offset},{len(info2)}\\n\")\n",
    "                merged_index_file.write(info2)\n",
    "\n",
    "                j += 1\n",
    "                offset += len(info2)\n",
    "            else:\n",
    "                with open(index1_path, \"r\") as index_file1:\n",
    "                    index_file1.seek(int(start1))\n",
    "                    info1 = index_file1.read(int(length1))\n",
    "                merged_catalog_file.write(f\"{term1}:{offset},{len(info1)}\\n\")\n",
    "                merged_index_file.write(info1)\n",
    "\n",
    "                i += 1\n",
    "                offset += len(info1)\n",
    "        while i < len(catalog1):\n",
    "            term, (start, length) = catalog1[i]\n",
    "            with open(index1_path, \"r\") as index_file1:\n",
    "                index_file1.seek(int(start))\n",
    "                info = index_file1.read(int(length))\n",
    "            merged_catalog_file.write(f\"{term}:{offset},{len(info)}\\n\")\n",
    "            merged_index_file.write(info)\n",
    "            \n",
    "            i += 1\n",
    "            offset += len(info)\n",
    "        while j < len(catalog2):\n",
    "            term, (start, length) = catalog2[j]\n",
    "            with open(index2_path, \"r\") as index_file2:\n",
    "                index_file2.seek(int(start))\n",
    "                info = index_file2.read(int(length))\n",
    "            merged_catalog_file.write(f\"{term}:{offset},{len(info)}\\n\")\n",
    "            merged_index_file.write(info)\n",
    "        \n",
    "            j += 1\n",
    "            offset += len(info)\n",
    "\n",
    "def master_merger_without_compression(stem=True):\n",
    "    if stem:\n",
    "        catalogs_path = STEMMED_CATALOG_PATH\n",
    "        indices_path = STEMMED_INDEX_PATH\n",
    "        merging_path = STEMMED_MERGE_PATH\n",
    "        decompressed_path = STEMMED_DECOMPRESSED_PATH\n",
    "    else:\n",
    "        catalogs_path = UNSTEMMED_CATALOG_PATH\n",
    "        indices_path = UNSTEMMED_INDEX_PATH\n",
    "        merging_path = UNSTEMMED_MERGE_PATH\n",
    "        decompressed_path = UNSTEMMED_DECOMPRESSED_PATH\n",
    "\n",
    "    os.system(f\"rm -r {merging_path}*\")\n",
    "\n",
    "    catalog_files = list_files_in_directory(catalogs_path)\n",
    "    index_files = list_files_in_directory(indices_path)\n",
    "    catalog_files.sort()\n",
    "    index_files.sort()\n",
    "\n",
    "    full_catalog_file = catalog_files[0]\n",
    "    full_catalog = load_catalog_from_file(full_catalog_file)\n",
    "    full_catalog_items = list(full_catalog.items())\n",
    "\n",
    "    full_index_file = index_files[0]\n",
    "\n",
    "    merge_count = 1\n",
    "\n",
    "    for i in tqdm(range(1, len(catalog_files))):\n",
    "        partial_catalog_file = catalog_files[i]\n",
    "        partial_catalog = load_catalog_from_file(partial_catalog_file)\n",
    "        partial_catalog_items = list(partial_catalog.items())\n",
    "        partial_index_file = index_files[i]\n",
    "\n",
    "        merge_indices_and_catalog_without_compression(merging_path, merge_count, full_catalog_items, full_index_file, partial_catalog_items, partial_index_file)\n",
    "\n",
    "        full_catalog_file = merging_path + \"merged\" + str(merge_count) + \"_catalog.txt\"\n",
    "        full_catalog = load_catalog_from_file(full_catalog_file)\n",
    "        full_catalog_items = list(full_catalog.items())\n",
    "        full_index_file = merging_path + \"merged\" + str(merge_count) + \"_index.txt\"\n",
    "\n",
    "        merge_count += 1\n",
    "\n",
    "    print(\"Copied merged files to compressed folder\")\n",
    "    os.system(f\"cp {merging_path}merged{merge_count-1}_catalog.txt {decompressed_path}\")\n",
    "    os.system(f\"cp {merging_path}merged{merge_count-1}_index.txt {decompressed_path}\")\n",
    "\n",
    "    print(\"Merging process completed.\")\n",
    "\n",
    "print(\"Merging stemmed indices...\")\n",
    "master_merger_without_compression(stem=True)\n",
    "print(\"Merging unstemmed indices...\")\n",
    "master_merger_without_compression(stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging stemmed indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [07:57<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied merged files to compressed folder\n",
      "Merging process completed.\n",
      "Merging unstemmed indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [08:35<00:00,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied merged files to compressed folder\n",
      "Merging process completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_indices(merge_directory, merge_number, catalog_list_a, index_path_a, catalog_list_b, index_path_b):\n",
    "    merged_catalog_file = open(merge_directory + \"merged\" + str(merge_number) + \"_catalog.txt\", \"x\")\n",
    "    merged_index_file = open(merge_directory + \"merged\" + str(merge_number) + \"_index.txt\", \"wb\")\n",
    "    i, j = 0, 0\n",
    "    offset = 0\n",
    "\n",
    "    while i < len(catalog_list_a) and j < len(catalog_list_b):\n",
    "        term1, (start1, length1) = catalog_list_a[i]\n",
    "        term2, (start2, length2) = catalog_list_b[j]\n",
    "\n",
    "        if term1 == term2:\n",
    "            if merge_number == 1:\n",
    "                file_a = open(index_path_a, \"r\")\n",
    "            else:\n",
    "                file_a = open(index_path_a, \"rb\")\n",
    "            file_b = open(index_path_b, \"r\")\n",
    "\n",
    "            file_a.seek(int(start1))\n",
    "            file_b.seek(int(start2))\n",
    "\n",
    "            pointer_a_doc_info = file_a.read(int(length1))\n",
    "            pointer_b_doc_info = file_b.read(int(length2))\n",
    "\n",
    "            file_a.close()\n",
    "            file_b.close()\n",
    "\n",
    "            if merge_number != 1:\n",
    "                pointer_a_doc_info = zlib.decompress(pointer_a_doc_info)\n",
    "                pointer_a_doc_info = str(pointer_a_doc_info, 'utf-8')\n",
    "\n",
    "            new_info = pointer_a_doc_info + pointer_b_doc_info\n",
    "            new_info_compressed = zlib.compress(new_info.encode('utf-8'), 6)\n",
    "\n",
    "            merged_catalog_file.write(str(term1) + \":\" + str(offset).strip() + \",\" + str(len(new_info_compressed)) + \"\\n\")\n",
    "            merged_index_file.write(new_info_compressed)\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "            offset += len(new_info_compressed)\n",
    "\n",
    "        elif term1 > term2:\n",
    "\n",
    "            file_b = open(index_path_b, \"r\")\n",
    "            file_b.seek(int(start2))\n",
    "            pointer_b_doc_info = file_b.read(int(length2))\n",
    "            file_b.close()\n",
    "\n",
    "            pointer_b_doc_info_compressed = zlib.compress(pointer_b_doc_info.encode('utf-8'), 6)\n",
    "\n",
    "            merged_catalog_file.write(str(term2) + \":\" + str(offset).strip() + \",\" + str(len(pointer_b_doc_info_compressed)) + \"\\n\")\n",
    "            merged_index_file.write(pointer_b_doc_info_compressed)\n",
    "\n",
    "            j += 1\n",
    "            offset += len(pointer_b_doc_info_compressed)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if merge_number == 1:\n",
    "                file_a = open(index_path_a, \"r\")\n",
    "            else:\n",
    "                file_a = open(index_path_a, \"rb\")\n",
    "\n",
    "            file_a.seek(int(start1))\n",
    "            pointer_a_doc_info = file_a.read(int(length1))\n",
    "            file_a.close()\n",
    "\n",
    "            if merge_number == 1:\n",
    "                pointer_a_doc_info = zlib.compress(pointer_a_doc_info.encode('utf-8'), 6)\n",
    "\n",
    "            merged_catalog_file.write(str(term1) + \":\" + str(offset).strip() + \",\" + str(len(pointer_a_doc_info)) + \"\\n\")\n",
    "            merged_index_file.write(pointer_a_doc_info)\n",
    "\n",
    "            i += 1\n",
    "            offset += len(pointer_a_doc_info)\n",
    "\n",
    "    while i < len(catalog_list_a):\n",
    "        term, (start, length) = catalog_list_a[i]\n",
    "\n",
    "        if merge_number == 1:\n",
    "            file_a = open(index_path_a, \"r\")\n",
    "        else:\n",
    "            file_a = open(index_path_a, \"rb\")\n",
    "\n",
    "        file_a.seek(int(start))\n",
    "        pointer_a_doc_info = file_a.read(int(length))\n",
    "        file_a.close()\n",
    "\n",
    "        if merge_number == 1:\n",
    "            pointer_a_doc_info = zlib.compress(pointer_a_doc_info.encode('utf-8'), 6)\n",
    "\n",
    "        merged_catalog_file.write(str(term) + \":\" + str(offset).strip() + \",\" + str(len(pointer_a_doc_info)) + \"\\n\")\n",
    "        merged_index_file.write(pointer_a_doc_info)\n",
    "\n",
    "        i += 1\n",
    "        offset += len(pointer_a_doc_info)\n",
    "\n",
    "    while j < len(catalog_list_b):\n",
    "        term, (start, length) = catalog_list_b[j]\n",
    "\n",
    "        file_b = open(index_path_b, \"r\")\n",
    "        file_b.seek(int(start))\n",
    "        pointer_b_doc_info = file_b.read(int(length))\n",
    "        file_b.close()\n",
    "\n",
    "        pointer_b_doc_info_compressed = zlib.compress(pointer_b_doc_info.encode('utf-8'), 6)\n",
    "\n",
    "        merged_catalog_file.write(str(term) + \":\" + str(offset).strip() + \",\" + str(len(pointer_b_doc_info_compressed)) + \"\\n\")\n",
    "        merged_index_file.write(pointer_b_doc_info_compressed)\n",
    "\n",
    "        j += 1\n",
    "        offset += len(pointer_b_doc_info_compressed)\n",
    "\n",
    "    merged_catalog_file.close()\n",
    "    merged_index_file.close()\n",
    "\n",
    "def master_merger(stem=True):\n",
    "    if stem:\n",
    "        catalogs_path = STEMMED_CATALOG_PATH\n",
    "        indices_path = STEMMED_INDEX_PATH\n",
    "        merging_path = STEMMED_MERGE_PATH\n",
    "        compressed_path = STEMMED_COMPRESSED_PATH\n",
    "    else:\n",
    "        catalogs_path = UNSTEMMED_CATALOG_PATH\n",
    "        indices_path = UNSTEMMED_INDEX_PATH\n",
    "        merging_path = UNSTEMMED_MERGE_PATH\n",
    "        compressed_path = UNSTEMMED_COMPRESSED_PATH\n",
    "    \n",
    "    os.system(f\"rm -r {merging_path}*\")\n",
    "\n",
    "    catalog_files = list_files_in_directory(catalogs_path)\n",
    "    index_files = list_files_in_directory(indices_path)\n",
    "    catalog_files.sort()\n",
    "    index_files.sort()\n",
    "\n",
    "    full_catalog_file = catalog_files[0]\n",
    "    full_catalog = load_catalog_from_file(full_catalog_file)\n",
    "    full_catalog_items = list(full_catalog.items())\n",
    "\n",
    "    full_index_file = index_files[0]\n",
    "\n",
    "    merge_count = 1\n",
    "\n",
    "    for i in tqdm(range(1, len(catalog_files))):\n",
    "        partial_catalog_file = catalog_files[i]\n",
    "        partial_catalog = load_catalog_from_file(partial_catalog_file)\n",
    "        partial_catalog_items = list(partial_catalog.items())\n",
    "        partial_index_file = index_files[i]\n",
    "\n",
    "        merge_indices(merging_path, merge_count, full_catalog_items, full_index_file, partial_catalog_items, partial_index_file)\n",
    "\n",
    "        full_catalog_file = merging_path + \"merged\" + str(merge_count) + \"_catalog.txt\"\n",
    "        full_catalog = load_catalog_from_file(full_catalog_file)\n",
    "        full_catalog_items = list(full_catalog.items())\n",
    "        full_index_file = merging_path + \"merged\" + str(merge_count) + \"_index.txt\"\n",
    "\n",
    "        merge_count += 1\n",
    "\n",
    "    print(\"Copied merged files to compressed folder\")\n",
    "    os.system(f\"cp {merging_path}merged{merge_count-1}_catalog.txt {compressed_path}\")\n",
    "    os.system(f\"cp {merging_path}merged{merge_count-1}_index.txt {compressed_path}\")\n",
    "\n",
    "    print(\"Merging process completed.\")\n",
    "\n",
    "print(\"Merging stemmed indices...\")\n",
    "master_merger(stem=True)\n",
    "print(\"Merging unstemmed indices...\")\n",
    "master_merger(stem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 : Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(stem=True):\n",
    "    stop_words = set()\n",
    "    with open(STOP_WORDS_PATH, 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "    \n",
    "    with open(QUERY_PATH, 'r') as file:\n",
    "        queries = file.readlines()\n",
    "\n",
    "    queries = {query.split()[0][:-1]: ' '.join(query.split()[1:]).strip() for query in queries}\n",
    "    if stem:\n",
    "        queries = {query_no: preprocess_text(query, stop_words, stem=True) for query_no, query in queries.items()}\n",
    "    else:\n",
    "        queries = {query_no: preprocess_text(query, stop_words, stem=False) for query_no, query in queries.items()}\n",
    "\n",
    "    return queries\n",
    "\n",
    "stemmed_queries = process_queries(stem=True)\n",
    "unstemmed_queries = process_queries(stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMED_INVERTED_INDEX_WITH_COMPRESSION = STEMMED_COMPRESSED_PATH + \"merged84_index.txt\"\n",
    "UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION = UNSTEMMED_COMPRESSED_PATH + \"merged84_index.txt\"\n",
    "\n",
    "STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION = STEMMED_DECOMPRESSED_PATH + \"merged84_index.txt\"\n",
    "UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION = UNSTEMMED_DECOMPRESSED_PATH + \"merged84_index.txt\"\n",
    "\n",
    "STEMMED_CATALOG_WITH_COMPRESSION = load_catalog_from_file(STEMMED_COMPRESSED_PATH + \"merged84_catalog.txt\")\n",
    "UNSTEMMED_CATALOG_WITH_COMPRESSION = load_catalog_from_file(UNSTEMMED_COMPRESSED_PATH + \"merged84_catalog.txt\")\n",
    "\n",
    "STEMMED_CATALOG_WITHOUT_COMPRESSION = load_catalog_from_file(STEMMED_DECOMPRESSED_PATH + \"merged84_catalog.txt\")\n",
    "UNSTEMMED_CATALOG_WITHOUT_COMPRESSION = load_catalog_from_file(UNSTEMMED_DECOMPRESSED_PATH + \"merged84_catalog.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_parser(doc_info):\n",
    "    doc_dict = {}\n",
    "    for doc in doc_info.strip().split(\"|\")[:-1]:\n",
    "        doc_no, positions = doc.split(\":\")\n",
    "        doc_dict[doc_no] = list(map(int, positions.split(\",\")))\n",
    "    return doc_dict\n",
    "\n",
    "def write_scores(scores_dict, file_name):\n",
    "    with open(file_name + \".txt\", \"w\") as score_file:\n",
    "        for i, (query_id, doc_scores) in enumerate(scores_dict.items(),1):\n",
    "            for doc_id, score in doc_scores.items():\n",
    "                if i > 1000:\n",
    "                    break\n",
    "                line = f\"{query_id} Q0 {DOC_NO_MAP[doc_id]} {i+1} {score} Exp\\n\"\n",
    "                score_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def okapi_score(queries, stem=True, compression=True):\n",
    "#     if stem:\n",
    "#         if compression:\n",
    "#             INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "#             FULL_CATALOG = STEMMED_CATALOG_WITH_COMPRESSION\n",
    "#         else:\n",
    "#             INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION        \n",
    "#             FULL_CATALOG = STEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "#         doc_len_map = DOC_LEN_MAP_STEM\n",
    "#     else:\n",
    "#         if compression:\n",
    "#             INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "#             FULL_CATALOG = UNSTEMMED_CATALOG_WITH_COMPRESSION\n",
    "#         else:\n",
    "#             INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION\n",
    "#             FULL_CATALOG = UNSTEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "#         doc_len_map = DOC_LEN_MAP_UNSTEM\n",
    "\n",
    "#     file = open(INVERTED_INDEX, \"rb\" if compression else \"r\")\n",
    "        \n",
    "#     scores = {q_id: {doc: 0 for doc in DOC_NO_MAP.keys()} for q_id, query in queries.items()}\n",
    "#     for q_id, query in tqdm(queries.items()):\n",
    "#         for word in query.split():\n",
    "#             if word in FULL_CATALOG:\n",
    "#                 start, length = FULL_CATALOG[word]\n",
    "\n",
    "#                 file.seek(int(start))\n",
    "#                 doc_info = file.read(int(length))\n",
    "#                 if compression:\n",
    "#                     doc_dict = index_parser(str(zlib.decompress(doc_info), 'utf-8'))\n",
    "#                 else:\n",
    "#                     doc_dict = index_parser(doc_info)\n",
    "\n",
    "#                 for doc, positions in doc_dict.items():\n",
    "#                     tf_wd = len(positions)\n",
    "#                     curr_doc_len = DOC_LEN_MAP_STEM[int(doc)]\n",
    "#                     avg_doc_len = sum(DOC_LEN_MAP_STEM.values()) / len(DOC_LEN_MAP_STEM)\n",
    "#                     scores[q_id][int(doc)] += okapi_tf_formula(tf_wd, curr_doc_len, avg_doc_len)\n",
    "#         scores[q_id] = {doc: score for doc, score in sorted(scores[q_id].items(), key=lambda item: item[1], reverse=True)}\n",
    "#     file.close()\n",
    "\n",
    "#     return scores\n",
    "\n",
    "# print(\"OKAPI TF SCORES STEMMED INDEX WITH COMPRESSION\")\n",
    "# okapi_scores = okapi_score(queries, stem=True, compression=True)\n",
    "# write_scores(okapi_scores, STEMMED_PATH + \"results/okapi_scores\")\n",
    "# !perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/okapi_scores.txt\n",
    "\n",
    "# print(\"OKAPI TF SCORES UNSTEMMED INDEX WITH COMPRESSION\")\n",
    "# okapi_scores_us = okapi_score(queries, stem=False, compression=True)\n",
    "# write_scores(okapi_scores_us, UNSTEMMED_PATH + \"results/okapi_scores_us\")\n",
    "# !perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/okapi_scores_us.txt\n",
    "\n",
    "# print(\"OKAPI TF SCORES STEMMED INDEX WITHOUT COMPRESSION\")\n",
    "# okapi_scores_wc = okapi_score(queries, stem=True, compression=False)\n",
    "# write_scores(okapi_scores_wc, STEMMED_PATH + \"results/okapi_scores_wc\")\n",
    "# !perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/okapi_scores_wc.txt\n",
    "\n",
    "# print(\"OKAPI TF SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\")\n",
    "# okapi_scores_us_wc = okapi_score(queries, stem=False, compression=False)\n",
    "# write_scores(okapi_scores_us_wc, UNSTEMMED_PATH + \"results/okapi_scores_us_wc\")\n",
    "# !perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/okapi_scores_us_wc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF SCORES STEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [17:10<00:00, 41.21s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1194\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6419\n",
      "    at 0.10       0.5171\n",
      "    at 0.20       0.3782\n",
      "    at 0.30       0.3329\n",
      "    at 0.40       0.2902\n",
      "    at 0.50       0.2519\n",
      "    at 0.60       0.2098\n",
      "    at 0.70       0.1770\n",
      "    at 0.80       0.1127\n",
      "    at 0.90       0.0538\n",
      "    at 1.00       0.0125\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2570\n",
      "Precision:\n",
      "  At    5 docs:   0.4320\n",
      "  At   10 docs:   0.3960\n",
      "  At   15 docs:   0.3893\n",
      "  At   20 docs:   0.3740\n",
      "  At   30 docs:   0.3520\n",
      "  At  100 docs:   0.2248\n",
      "  At  200 docs:   0.1596\n",
      "  At  500 docs:   0.0830\n",
      "  At 1000 docs:   0.0478\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2885\n",
      "TF-IDF SCORES UNSTEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:26<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1015\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6619\n",
      "    at 0.10       0.4840\n",
      "    at 0.20       0.3058\n",
      "    at 0.30       0.2621\n",
      "    at 0.40       0.2259\n",
      "    at 0.50       0.2002\n",
      "    at 0.60       0.1660\n",
      "    at 0.70       0.1340\n",
      "    at 0.80       0.0784\n",
      "    at 0.90       0.0394\n",
      "    at 1.00       0.0083\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2126\n",
      "Precision:\n",
      "  At    5 docs:   0.4000\n",
      "  At   10 docs:   0.3280\n",
      "  At   15 docs:   0.3120\n",
      "  At   20 docs:   0.2960\n",
      "  At   30 docs:   0.2747\n",
      "  At  100 docs:   0.1896\n",
      "  At  200 docs:   0.1300\n",
      "  At  500 docs:   0.0694\n",
      "  At 1000 docs:   0.0406\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2342\n",
      "TF-IDF SCORES STEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [06:50<00:00, 16.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1194\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6419\n",
      "    at 0.10       0.5171\n",
      "    at 0.20       0.3782\n",
      "    at 0.30       0.3329\n",
      "    at 0.40       0.2902\n",
      "    at 0.50       0.2519\n",
      "    at 0.60       0.2098\n",
      "    at 0.70       0.1770\n",
      "    at 0.80       0.1127\n",
      "    at 0.90       0.0538\n",
      "    at 1.00       0.0125\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2570\n",
      "Precision:\n",
      "  At    5 docs:   0.4320\n",
      "  At   10 docs:   0.3960\n",
      "  At   15 docs:   0.3893\n",
      "  At   20 docs:   0.3740\n",
      "  At   30 docs:   0.3520\n",
      "  At  100 docs:   0.2248\n",
      "  At  200 docs:   0.1596\n",
      "  At  500 docs:   0.0830\n",
      "  At 1000 docs:   0.0478\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2885\n",
      "TF-IDF SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:16<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1015\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6619\n",
      "    at 0.10       0.4840\n",
      "    at 0.20       0.3058\n",
      "    at 0.30       0.2621\n",
      "    at 0.40       0.2259\n",
      "    at 0.50       0.2002\n",
      "    at 0.60       0.1660\n",
      "    at 0.70       0.1340\n",
      "    at 0.80       0.0784\n",
      "    at 0.90       0.0394\n",
      "    at 1.00       0.0083\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2126\n",
      "Precision:\n",
      "  At    5 docs:   0.4000\n",
      "  At   10 docs:   0.3280\n",
      "  At   15 docs:   0.3120\n",
      "  At   20 docs:   0.2960\n",
      "  At   30 docs:   0.2747\n",
      "  At  100 docs:   0.1896\n",
      "  At  200 docs:   0.1300\n",
      "  At  500 docs:   0.0694\n",
      "  At 1000 docs:   0.0406\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2342\n"
     ]
    }
   ],
   "source": [
    "def okapi_tf_formula(tf_wd, curr_doc_len, avg_doc_len):\n",
    "    return tf_wd / (tf_wd + 0.5 + (1.5 * (curr_doc_len / avg_doc_len)))\n",
    "\n",
    "def tf_idf_score(queries, stem=True, compression=True):\n",
    "    if stem:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION        \n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_STEM\n",
    "    else:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_UNSTEM\n",
    "\n",
    "    file = open(INVERTED_INDEX, \"rb\" if compression else \"r\")\n",
    "\n",
    "    scores = {q_id: {doc: 0 for doc in DOC_NO_MAP.keys()} for q_id, query in queries.items()}\n",
    "    for q_id, query in tqdm(queries.items()):\n",
    "        for word in query.split():\n",
    "            if stem:\n",
    "                word_key = STEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "            else:\n",
    "                word_key = UNSTEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "\n",
    "            if word_key in FULL_CATALOG:\n",
    "                start, length = FULL_CATALOG[word_key]\n",
    "                file.seek(int(start))\n",
    "                doc_info = file.read(int(length))\n",
    "                if compression:\n",
    "                    doc_dict = index_parser(str(zlib.decompress(doc_info), 'utf-8'))\n",
    "                else:\n",
    "                    doc_dict = index_parser(doc_info)\n",
    "\n",
    "                for doc, positions in doc_dict.items():\n",
    "                    tf_wd = len(positions)\n",
    "                    curr_doc_len = doc_len_map[int(doc)]\n",
    "                    avg_doc_len = sum(doc_len_map.values()) / len(DOC_LEN_MAP_STEM)\n",
    "                    doc_freq = len(doc_dict)\n",
    "                    number_of_documents = len(DOC_NO_MAP)\n",
    "                    scores[q_id][int(doc)] += okapi_tf_formula(tf_wd, curr_doc_len, avg_doc_len) * math.log(number_of_documents / doc_freq)\n",
    "        scores[q_id] = {doc: score for doc, score in sorted(scores[q_id].items(), key=lambda item: item[1], reverse=True)}\n",
    "    file.close()\n",
    "    return scores\n",
    "\n",
    "print(\"TF-IDF SCORES STEMMED INDEX WITH COMPRESSION\")\n",
    "tf_idf_scores = tf_idf_score(stemmed_queries, stem=True, compression=True)\n",
    "write_scores(tf_idf_scores, STEMMED_PATH + \"results/tf_idf_scores\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/tf_idf_scores.txt\n",
    "\n",
    "print(\"TF-IDF SCORES UNSTEMMED INDEX WITH COMPRESSION\")\n",
    "tf_idf_scores_us = tf_idf_score(unstemmed_queries, stem=False, compression=True)\n",
    "write_scores(tf_idf_scores_us, UNSTEMMED_PATH + \"results/tf_idf_scores_us\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/tf_idf_scores_us.txt\n",
    "\n",
    "print(\"TF-IDF SCORES STEMMED INDEX WITHOUT COMPRESSION\")\n",
    "tf_idf_scores_wc = tf_idf_score(stemmed_queries, stem=True, compression=False)\n",
    "write_scores(tf_idf_scores_wc, STEMMED_PATH + \"results/tf_idf_scores_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/tf_idf_scores_wc.txt\n",
    "\n",
    "print(\"TF-IDF SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\")\n",
    "tf_idf_scores_us_wc = tf_idf_score(unstemmed_queries, stem=False, compression=False)\n",
    "write_scores(tf_idf_scores_us_wc, UNSTEMMED_PATH + \"results/tf_idf_scores_us_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/tf_idf_scores_us_wc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 SCORES STEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [07:29<00:00, 17.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1197\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6863\n",
      "    at 0.10       0.5343\n",
      "    at 0.20       0.4075\n",
      "    at 0.30       0.3408\n",
      "    at 0.40       0.2944\n",
      "    at 0.50       0.2572\n",
      "    at 0.60       0.2117\n",
      "    at 0.70       0.1873\n",
      "    at 0.80       0.1323\n",
      "    at 0.90       0.0517\n",
      "    at 1.00       0.0139\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2665\n",
      "Precision:\n",
      "  At    5 docs:   0.4480\n",
      "  At   10 docs:   0.4040\n",
      "  At   15 docs:   0.3867\n",
      "  At   20 docs:   0.3840\n",
      "  At   30 docs:   0.3600\n",
      "  At  100 docs:   0.2340\n",
      "  At  200 docs:   0.1608\n",
      "  At  500 docs:   0.0848\n",
      "  At 1000 docs:   0.0479\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2952\n",
      "BM25 SCORES UNSTEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:36<00:00, 11.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1037\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.7079\n",
      "    at 0.10       0.4879\n",
      "    at 0.20       0.3187\n",
      "    at 0.30       0.2593\n",
      "    at 0.40       0.2358\n",
      "    at 0.50       0.2089\n",
      "    at 0.60       0.1764\n",
      "    at 0.70       0.1446\n",
      "    at 0.80       0.0965\n",
      "    at 0.90       0.0570\n",
      "    at 1.00       0.0124\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2214\n",
      "Precision:\n",
      "  At    5 docs:   0.4000\n",
      "  At   10 docs:   0.3520\n",
      "  At   15 docs:   0.3200\n",
      "  At   20 docs:   0.3000\n",
      "  At   30 docs:   0.2827\n",
      "  At  100 docs:   0.1928\n",
      "  At  200 docs:   0.1334\n",
      "  At  500 docs:   0.0713\n",
      "  At 1000 docs:   0.0415\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2464\n",
      "BM25 SCORES STEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [07:06<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1197\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6863\n",
      "    at 0.10       0.5343\n",
      "    at 0.20       0.4075\n",
      "    at 0.30       0.3408\n",
      "    at 0.40       0.2944\n",
      "    at 0.50       0.2572\n",
      "    at 0.60       0.2117\n",
      "    at 0.70       0.1873\n",
      "    at 0.80       0.1323\n",
      "    at 0.90       0.0517\n",
      "    at 1.00       0.0139\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2665\n",
      "Precision:\n",
      "  At    5 docs:   0.4480\n",
      "  At   10 docs:   0.4040\n",
      "  At   15 docs:   0.3867\n",
      "  At   20 docs:   0.3840\n",
      "  At   30 docs:   0.3600\n",
      "  At  100 docs:   0.2340\n",
      "  At  200 docs:   0.1608\n",
      "  At  500 docs:   0.0848\n",
      "  At 1000 docs:   0.0479\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2952\n",
      "BM25 SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:36<00:00, 11.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1037\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.7079\n",
      "    at 0.10       0.4879\n",
      "    at 0.20       0.3187\n",
      "    at 0.30       0.2593\n",
      "    at 0.40       0.2358\n",
      "    at 0.50       0.2089\n",
      "    at 0.60       0.1764\n",
      "    at 0.70       0.1446\n",
      "    at 0.80       0.0965\n",
      "    at 0.90       0.0570\n",
      "    at 1.00       0.0124\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2214\n",
      "Precision:\n",
      "  At    5 docs:   0.4000\n",
      "  At   10 docs:   0.3520\n",
      "  At   15 docs:   0.3200\n",
      "  At   20 docs:   0.3000\n",
      "  At   30 docs:   0.2827\n",
      "  At  100 docs:   0.1928\n",
      "  At  200 docs:   0.1334\n",
      "  At  500 docs:   0.0713\n",
      "  At 1000 docs:   0.0415\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2464\n"
     ]
    }
   ],
   "source": [
    "def bm25_score(queries, stem=True, compression=True):\n",
    "    if stem:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION        \n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_STEM\n",
    "    else:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_UNSTEM\n",
    "    \n",
    "    file = open(INVERTED_INDEX, \"rb\" if compression else \"r\")\n",
    "\n",
    "    scores = {q_id: {doc: 0 for doc in DOC_NO_MAP.keys()} for q_id, query in queries.items()}\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    for q_id, query in tqdm(queries.items()):\n",
    "        for word in query.split():\n",
    "            if stem:\n",
    "                word_key = STEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "            else:\n",
    "                word_key = UNSTEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "\n",
    "            if word_key in FULL_CATALOG:\n",
    "                start, length = FULL_CATALOG[word_key]\n",
    "                file.seek(int(start))\n",
    "                doc_info = file.read(int(length))\n",
    "                if compression:\n",
    "                    doc_dict = index_parser(str(zlib.decompress(doc_info), 'utf-8'))\n",
    "                else:\n",
    "                    doc_dict = index_parser(doc_info)\n",
    "\n",
    "                for doc, positions in doc_dict.items():\n",
    "                    tf_wd = len(positions)\n",
    "                    curr_doc_len = doc_len_map[int(doc)]\n",
    "                    avg_doc_len = sum(doc_len_map.values()) / len(doc_len_map)\n",
    "                    doc_freq = len(doc_dict)\n",
    "                    number_of_documents = len(DOC_NO_MAP)\n",
    "                    idf = math.log((number_of_documents - doc_freq + 0.5) / (doc_freq + 0.5))\n",
    "                    bm25 = idf * ((tf_wd * (k1 + 1)) / (tf_wd + k1 * (1 - b + b * (curr_doc_len / avg_doc_len))))\n",
    "                    scores[q_id][int(doc)] += bm25\n",
    "        scores[q_id] = {doc: score for doc, score in sorted(scores[q_id].items(), key=lambda item: item[1], reverse=True)}\n",
    "    file.close()\n",
    "    return scores\n",
    "\n",
    "print(\"BM25 SCORES STEMMED INDEX WITH COMPRESSION\")\n",
    "bm25_scores = bm25_score(stemmed_queries, stem=True, compression=True)\n",
    "write_scores(bm25_scores, STEMMED_PATH + \"results/bm25_scores\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/bm25_scores.txt\n",
    "\n",
    "print(\"BM25 SCORES UNSTEMMED INDEX WITH COMPRESSION\")\n",
    "bm25_scores_us = bm25_score(unstemmed_queries, stem=False, compression=True)\n",
    "write_scores(bm25_scores_us, UNSTEMMED_PATH + \"results/bm25_scores_us\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/bm25_scores_us.txt\n",
    "\n",
    "print(\"BM25 SCORES STEMMED INDEX WITHOUT COMPRESSION\")\n",
    "bm25_scores_wc = bm25_score(stemmed_queries, stem=True, compression=False)\n",
    "write_scores(bm25_scores_wc, STEMMED_PATH + \"results/bm25_scores_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/bm25_scores_wc.txt\n",
    "\n",
    "print(\"BM25 SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\")\n",
    "bm25_scores_us_wc = bm25_score(unstemmed_queries, stem=False, compression=False)\n",
    "write_scores(bm25_scores_us_wc, UNSTEMMED_PATH + \"results/bm25_scores_us_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/bm25_scores_us_wc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM LAPLACE SMOOTH SCORES STEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1094\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6153\n",
      "    at 0.10       0.4866\n",
      "    at 0.20       0.4010\n",
      "    at 0.30       0.3156\n",
      "    at 0.40       0.2414\n",
      "    at 0.50       0.1898\n",
      "    at 0.60       0.1450\n",
      "    at 0.70       0.1086\n",
      "    at 0.80       0.0665\n",
      "    at 0.90       0.0258\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2180\n",
      "Precision:\n",
      "  At    5 docs:   0.4240\n",
      "  At   10 docs:   0.4080\n",
      "  At   15 docs:   0.3680\n",
      "  At   20 docs:   0.3420\n",
      "  At   30 docs:   0.3093\n",
      "  At  100 docs:   0.2076\n",
      "  At  200 docs:   0.1438\n",
      "  At  500 docs:   0.0744\n",
      "  At 1000 docs:   0.0438\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2613\n",
      "LM LAPLACE SMOOTH SCORES UNSTEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1003\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6478\n",
      "    at 0.10       0.4441\n",
      "    at 0.20       0.3376\n",
      "    at 0.30       0.2787\n",
      "    at 0.40       0.2143\n",
      "    at 0.50       0.1811\n",
      "    at 0.60       0.1328\n",
      "    at 0.70       0.0993\n",
      "    at 0.80       0.0689\n",
      "    at 0.90       0.0298\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.1995\n",
      "Precision:\n",
      "  At    5 docs:   0.3600\n",
      "  At   10 docs:   0.3480\n",
      "  At   15 docs:   0.3227\n",
      "  At   20 docs:   0.3120\n",
      "  At   30 docs:   0.2787\n",
      "  At  100 docs:   0.1900\n",
      "  At  200 docs:   0.1290\n",
      "  At  500 docs:   0.0680\n",
      "  At 1000 docs:   0.0401\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2446\n",
      "LM LAPLACE SMOOTH SCORES STEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1094\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6153\n",
      "    at 0.10       0.4866\n",
      "    at 0.20       0.4010\n",
      "    at 0.30       0.3156\n",
      "    at 0.40       0.2414\n",
      "    at 0.50       0.1898\n",
      "    at 0.60       0.1450\n",
      "    at 0.70       0.1086\n",
      "    at 0.80       0.0665\n",
      "    at 0.90       0.0258\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2180\n",
      "Precision:\n",
      "  At    5 docs:   0.4240\n",
      "  At   10 docs:   0.4080\n",
      "  At   15 docs:   0.3680\n",
      "  At   20 docs:   0.3420\n",
      "  At   30 docs:   0.3093\n",
      "  At  100 docs:   0.2076\n",
      "  At  200 docs:   0.1438\n",
      "  At  500 docs:   0.0744\n",
      "  At 1000 docs:   0.0438\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2613\n",
      "LM LAPLACE SMOOTH SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1003\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.6478\n",
      "    at 0.10       0.4441\n",
      "    at 0.20       0.3376\n",
      "    at 0.30       0.2787\n",
      "    at 0.40       0.2143\n",
      "    at 0.50       0.1811\n",
      "    at 0.60       0.1328\n",
      "    at 0.70       0.0993\n",
      "    at 0.80       0.0689\n",
      "    at 0.90       0.0298\n",
      "    at 1.00       0.0000\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.1995\n",
      "Precision:\n",
      "  At    5 docs:   0.3600\n",
      "  At   10 docs:   0.3480\n",
      "  At   15 docs:   0.3227\n",
      "  At   20 docs:   0.3120\n",
      "  At   30 docs:   0.2787\n",
      "  At  100 docs:   0.1900\n",
      "  At  200 docs:   0.1290\n",
      "  At  500 docs:   0.0680\n",
      "  At 1000 docs:   0.0401\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2446\n"
     ]
    }
   ],
   "source": [
    "def lm_laplace_smooth_score(queries, stem=True, compression=True):\n",
    "    if stem:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION        \n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_STEM\n",
    "    else:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_UNSTEM\n",
    "    \n",
    "    file = open(INVERTED_INDEX, \"rb\" if compression else \"r\")\n",
    "\n",
    "    scores = {q_id: {doc: 0 for doc in DOC_NO_MAP.keys()} for q_id, query in queries.items()}\n",
    "    # Find the vocabulary size\n",
    "    V = len(FULL_CATALOG)\n",
    "    for q_id, query in tqdm(queries.items()):\n",
    "        for word in query.split():\n",
    "            if stem:\n",
    "                word_key = STEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "            else:\n",
    "                word_key = UNSTEMMED_TOKEN_TO_WORD.get(word, \"MISSING\")\n",
    "\n",
    "            if word_key in FULL_CATALOG:            \n",
    "                start, length = FULL_CATALOG[word_key]\n",
    "                file.seek(int(start))\n",
    "                doc_info = file.read(int(length))\n",
    "                if compression:\n",
    "                    doc_dict = index_parser(str(zlib.decompress(doc_info), 'utf-8'))\n",
    "                else:\n",
    "                    doc_dict = index_parser(doc_info)\n",
    "                \n",
    "                for doc, positions in doc_dict.items():\n",
    "                    tf_wd = len(positions)\n",
    "                    curr_doc_len = doc_len_map[int(doc)]\n",
    "                    scores[q_id][int(doc)] += np.log((tf_wd + 1) / (curr_doc_len + V))\n",
    "                \n",
    "                for doc in DOC_NO_MAP.keys():\n",
    "                    if str(doc) not in doc_dict.keys():\n",
    "                        scores[q_id][doc] += -100\n",
    "\n",
    "        scores[q_id] = {doc: score for doc, score in sorted(scores[q_id].items(), key=lambda item: item[1], reverse=True)}\n",
    "    file.close()\n",
    "    return scores\n",
    "\n",
    "print(\"LM LAPLACE SMOOTH SCORES STEMMED INDEX WITH COMPRESSION\")\n",
    "lm_laplace_scores = lm_laplace_smooth_score(stemmed_queries, stem=True, compression=True)\n",
    "write_scores(lm_laplace_scores, STEMMED_PATH + \"results/lm_laplace_scores\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/lm_laplace_scores.txt\n",
    "\n",
    "print(\"LM LAPLACE SMOOTH SCORES UNSTEMMED INDEX WITH COMPRESSION\")\n",
    "lm_laplace_scores_us = lm_laplace_smooth_score(unstemmed_queries, stem=False, compression=True)\n",
    "write_scores(lm_laplace_scores_us, UNSTEMMED_PATH + \"results/lm_laplace_scores_us\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/lm_laplace_scores_us.txt\n",
    "\n",
    "print(\"LM LAPLACE SMOOTH SCORES STEMMED INDEX WITHOUT COMPRESSION\")\n",
    "lm_laplace_scores_wc = lm_laplace_smooth_score(stemmed_queries, stem=True, compression=False)\n",
    "write_scores(lm_laplace_scores_wc, STEMMED_PATH + \"results/lm_laplace_scores_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/lm_laplace_scores_wc.txt\n",
    "\n",
    "print(\"LM LAPLACE SMOOTH SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\")\n",
    "lm_laplace_scores_us_wc = lm_laplace_smooth_score(unstemmed_queries, stem=False, compression=False)\n",
    "write_scores(lm_laplace_scores_us_wc, UNSTEMMED_PATH + \"results/lm_laplace_scores_us_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/lm_laplace_scores_us_wc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROXIMITY SEARCH SCORES STEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [07:02<00:00, 16.89s/it]\n",
      "100%|██████████| 25/25 [00:06<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1198\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.7169\n",
      "    at 0.10       0.5059\n",
      "    at 0.20       0.4108\n",
      "    at 0.30       0.3504\n",
      "    at 0.40       0.2926\n",
      "    at 0.50       0.2580\n",
      "    at 0.60       0.2154\n",
      "    at 0.70       0.1883\n",
      "    at 0.80       0.1318\n",
      "    at 0.90       0.0544\n",
      "    at 1.00       0.0137\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2673\n",
      "Precision:\n",
      "  At    5 docs:   0.4640\n",
      "  At   10 docs:   0.4200\n",
      "  At   15 docs:   0.4053\n",
      "  At   20 docs:   0.3760\n",
      "  At   30 docs:   0.3493\n",
      "  At  100 docs:   0.2312\n",
      "  At  200 docs:   0.1612\n",
      "  At  500 docs:   0.0853\n",
      "  At 1000 docs:   0.0479\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2970\n",
      "PROXIMITY SEARCH SCORES UNSTEMMED INDEX WITH COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:07<00:00,  7.52s/it]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1087\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.4632\n",
      "    at 0.10       0.3149\n",
      "    at 0.20       0.2715\n",
      "    at 0.30       0.2343\n",
      "    at 0.40       0.1889\n",
      "    at 0.50       0.1601\n",
      "    at 0.60       0.1265\n",
      "    at 0.70       0.1066\n",
      "    at 0.80       0.0769\n",
      "    at 0.90       0.0526\n",
      "    at 1.00       0.0136\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.1698\n",
      "Precision:\n",
      "  At    5 docs:   0.3360\n",
      "  At   10 docs:   0.2800\n",
      "  At   15 docs:   0.2667\n",
      "  At   20 docs:   0.2500\n",
      "  At   30 docs:   0.2453\n",
      "  At  100 docs:   0.1712\n",
      "  At  200 docs:   0.1242\n",
      "  At  500 docs:   0.0716\n",
      "  At 1000 docs:   0.0435\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.1967\n",
      "PROXIMITY SEARCH SCORES STEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [07:15<00:00, 17.43s/it]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1198\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.7169\n",
      "    at 0.10       0.5059\n",
      "    at 0.20       0.4108\n",
      "    at 0.30       0.3504\n",
      "    at 0.40       0.2926\n",
      "    at 0.50       0.2580\n",
      "    at 0.60       0.2154\n",
      "    at 0.70       0.1883\n",
      "    at 0.80       0.1318\n",
      "    at 0.90       0.0544\n",
      "    at 1.00       0.0137\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.2673\n",
      "Precision:\n",
      "  At    5 docs:   0.4640\n",
      "  At   10 docs:   0.4200\n",
      "  At   15 docs:   0.4053\n",
      "  At   20 docs:   0.3760\n",
      "  At   30 docs:   0.3493\n",
      "  At  100 docs:   0.2312\n",
      "  At  200 docs:   0.1612\n",
      "  At  500 docs:   0.0853\n",
      "  At 1000 docs:   0.0479\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.2970\n",
      "PROXIMITY SEARCH SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:10<00:00,  7.61s/it]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error due to 25\n",
      "\n",
      "Queryid (Num):       25\n",
      "Total number of documents over all queries\n",
      "    Retrieved:    25000\n",
      "    Relevant:      1832\n",
      "    Rel_ret:       1087\n",
      "Interpolated Recall - Precision Averages:\n",
      "    at 0.00       0.4632\n",
      "    at 0.10       0.3149\n",
      "    at 0.20       0.2715\n",
      "    at 0.30       0.2343\n",
      "    at 0.40       0.1889\n",
      "    at 0.50       0.1601\n",
      "    at 0.60       0.1265\n",
      "    at 0.70       0.1066\n",
      "    at 0.80       0.0769\n",
      "    at 0.90       0.0526\n",
      "    at 1.00       0.0136\n",
      "Average precision (non-interpolated) for all rel docs(averaged over queries)\n",
      "                  0.1698\n",
      "Precision:\n",
      "  At    5 docs:   0.3360\n",
      "  At   10 docs:   0.2800\n",
      "  At   15 docs:   0.2667\n",
      "  At   20 docs:   0.2500\n",
      "  At   30 docs:   0.2453\n",
      "  At  100 docs:   0.1712\n",
      "  At  200 docs:   0.1242\n",
      "  At  500 docs:   0.0716\n",
      "  At 1000 docs:   0.0435\n",
      "R-Precision (precision after R (= num_rel for a query) docs retrieved):\n",
      "    Exact:        0.1967\n"
     ]
    }
   ],
   "source": [
    "def calculate_accumulator(total_no_of_docs, proximity_threshold, word, word_positions_dict):\n",
    "    current_positions = word_positions_dict.get(word)\n",
    "    accumulator_value = 0\n",
    "    for term, positions in word_positions_dict.items():\n",
    "        if term == word:\n",
    "            continue\n",
    "        term_positions = 0\n",
    "        current_term_positions = 0\n",
    "        term_j_positions = positions\n",
    "        while term_positions < len(current_positions) and current_term_positions < len(term_j_positions):\n",
    "            window = abs(int(current_positions[term_positions]) - int(term_j_positions[current_term_positions]))\n",
    "            if window <= proximity_threshold:\n",
    "                term_frequency = len(term_j_positions)\n",
    "                temp_accumulator = (math.log((total_no_of_docs) / (term_frequency + 1), 2)) / proximity_threshold ** 2\n",
    "                accumulator_value += temp_accumulator\n",
    "            if int(current_positions[term_positions]) < int(term_j_positions[current_term_positions]):\n",
    "                term_positions += 1\n",
    "            else:\n",
    "                current_term_positions += 1\n",
    "        while term_positions < len(current_positions):\n",
    "            window = abs(int(current_positions[term_positions]) - int(term_j_positions[current_term_positions - 1]))\n",
    "            if window <= proximity_threshold:\n",
    "                term_frequency = len(term_j_positions)\n",
    "                temp_accumulator = (math.log((total_no_of_docs) / (term_frequency + 1), 2)) / proximity_threshold ** 2\n",
    "                accumulator_value += temp_accumulator\n",
    "            term_positions += 1\n",
    "        while current_term_positions < len(term_j_positions):\n",
    "            window = abs(int(current_positions[term_positions - 1]) - int(term_j_positions[current_term_positions]))\n",
    "            if window <= proximity_threshold:\n",
    "                term_frequency = len(term_j_positions)\n",
    "                temp_accumulator = (math.log((total_no_of_docs) / (term_frequency + 1), 2)) / proximity_threshold ** 2\n",
    "                accumulator_value += temp_accumulator\n",
    "            current_term_positions += 1\n",
    "    return accumulator_value\n",
    "\n",
    "def calculate_proximity_score(no_of_docs, term_frequency, accumulator_term, document_length, average_document_length, k1, b):\n",
    "    inverse_document_frequency = math.log((no_of_docs) / (term_frequency + 1), 2)\n",
    "    calculation_2 = (accumulator_term * (k1 + 1)) / (accumulator_term + (k1 * ((1 - b) + (b * (document_length / average_document_length)))))\n",
    "    proximity_score = min(inverse_document_frequency, 1) * calculation_2\n",
    "    return proximity_score\n",
    "\n",
    "def proximity_search(query_dictionary, stem=True, compression=True):\n",
    "    if stem:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = STEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION        \n",
    "            FULL_CATALOG = STEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_STEM\n",
    "    else:\n",
    "        if compression:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITH_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITH_COMPRESSION\n",
    "        else:\n",
    "            INVERTED_INDEX = UNSTEMMED_INVERTED_INDEX_WITHOUT_COMPRESSION\n",
    "            FULL_CATALOG = UNSTEMMED_CATALOG_WITHOUT_COMPRESSION\n",
    "        doc_len_map = DOC_LEN_MAP_UNSTEM\n",
    "      \n",
    "    file = open(INVERTED_INDEX, \"rb\" if compression else \"r\")\n",
    "      \n",
    "    k1 = 1.5\n",
    "    b = 0.4\n",
    "    proximity_threshold = 2\n",
    "    average_document_length = sum(doc_len_map.values()) / len(doc_len_map)\n",
    "    scores = bm25_score(query_dictionary, doc_len_map, stem)\n",
    "    for query_id, query_terms in tqdm(query_dictionary.items()):\n",
    "        query_info = {}\n",
    "        for term in query_terms.split():\n",
    "            if stem:\n",
    "                word_key = STEMMED_TOKEN_TO_WORD.get(term, \"MISSING\")\n",
    "            else:\n",
    "                word_key = UNSTEMMED_TOKEN_TO_WORD.get(term, \"MISSING\")\n",
    "            term = word_key\n",
    "\n",
    "            index_placement = FULL_CATALOG.get(word_key, 0)\n",
    "            if index_placement != 0:\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "                file.seek(int(offset))\n",
    "                document_details = file.read(int(length))\n",
    "                if compression:\n",
    "                    document_details = index_parser(zlib.decompress(document_details).decode('utf-8'))\n",
    "                else:\n",
    "                    document_details = index_parser(document_details)\n",
    "\n",
    "                for doc_id, positions in document_details.items():\n",
    "                    if doc_id in query_info:\n",
    "                        query_info[doc_id][term] = positions\n",
    "                    else:\n",
    "                        query_info[doc_id] = {term: positions}\n",
    "\n",
    "        for doc_id, terms_positions in query_info.items():\n",
    "            for term, positions in terms_positions.items():\n",
    "                term_frequency = len(positions)\n",
    "                current_accumulator = calculate_accumulator(len(DOC_NO_MAP), proximity_threshold, term, terms_positions)\n",
    "                proximity_score = calculate_proximity_score(len(DOC_NO_MAP), term_frequency, current_accumulator, int(doc_id), average_document_length, k1, b)\n",
    "\n",
    "                scores[query_id][int(doc_id)] += proximity_score\n",
    "    file.close()\n",
    "    return scores\n",
    "\n",
    "print(\"PROXIMITY SEARCH SCORES STEMMED INDEX WITH COMPRESSION\")\n",
    "proximity_search_scores = proximity_search(stemmed_queries, stem=True, compression=True)\n",
    "write_scores(proximity_search_scores, STEMMED_PATH + \"results/proximity_search_scores\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/proximity_search_scores.txt\n",
    "\n",
    "print(\"PROXIMITY SEARCH SCORES UNSTEMMED INDEX WITH COMPRESSION\")\n",
    "proximity_search_scores_us = proximity_search(unstemmed_queries, stem=False, compression=True)\n",
    "write_scores(proximity_search_scores_us, UNSTEMMED_PATH + \"results/proximity_search_scores_us\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/proximity_search_scores_us.txt\n",
    "\n",
    "print(\"PROXIMITY SEARCH SCORES STEMMED INDEX WITHOUT COMPRESSION\")\n",
    "proximity_search_scores_wc = proximity_search(stemmed_queries, stem=True, compression=False)\n",
    "write_scores(proximity_search_scores_wc, STEMMED_PATH + \"results/proximity_search_scores_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt stemmed/results/proximity_search_scores_wc.txt\n",
    "\n",
    "print(\"PROXIMITY SEARCH SCORES UNSTEMMED INDEX WITHOUT COMPRESSION\")\n",
    "proximity_search_scores_us_wc = proximity_search(unstemmed_queries, stem=False, compression=False)\n",
    "write_scores(proximity_search_scores_us_wc, UNSTEMMED_PATH + \"results/proximity_search_scores_us_wc\")\n",
    "!perl ../trec_eval/trec_eval.pl ../AP_DATA/qrels.adhoc.51-100.AP89.txt unstemmed/results/proximity_search_scores_us_wc.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
