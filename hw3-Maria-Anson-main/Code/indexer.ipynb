{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from elasticsearch7 import Elasticsearch\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Link Graph\n",
    "You should also write a link graph reporting all out-links from each URL you crawl, all the inlinks you have encountered (obviously there will be inlinks on the web that you dont discover). This will be used in a future assignment to calculate PageRank for your collection.\n",
    "\n",
    "option 1 : We prefer that you store the canonical links as two fields “inlinks” and “outlinks” in ElasticSearch, for each document. You will have to manage these fields appropriately, such that when you are done, your team has correct links for all document crawled.\n",
    "\n",
    "option 2: maintain a separate links file (you can do this even if you also do option1). Each line of this file contains a tab-separated list of canonical URLs. The first URL is a document you crawled, and the remaining URLs are out-links from the document. When all team members are finished with their crawls, you should merge your link graphs. Only submit one file, containing this merged graph. During the merge process, reduce any URL which was not crawled to just a domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"trial_run\"\n",
    "document_name = os.path.join(path, \"documents.txt\")\n",
    "inlinks_name = os.path.join(path, \"inlinks_backup.json\")\n",
    "outlinks_name = os.path.join(path, \"outlinks.json\")\n",
    "\n",
    "INDEX_NAME = \"crawler\"\n",
    "es = Elasticsearch(cloud_id= \"0feeb24636464a578a9c7a1ce9739181:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyQyMzcyNjZmYzcwMzg0ZTA2OTM1MTJkZGIxMDgzYTRmMyQ1N2RhZjIzZTNiMWM0MjAwYjBhMDQ0MGY1ZTEyZTc2Yw==\",\n",
    "                   http_auth=(\"elastic\", \"pETnMazDlmfyCT2rZ2NAWh2V\"))\n",
    "\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCNO</th>\n",
       "      <th>URL</th>\n",
       "      <th>WAVENO</th>\n",
       "      <th>OUTLINKNO</th>\n",
       "      <th>INLINK_COUNT</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>INLINKS</th>\n",
       "      <th>OUTLINKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANSON-1</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2009_swine_flu_p...</td>\n",
       "      <td>0</td>\n",
       "      <td>602</td>\n",
       "      <td>2</td>\n",
       "      <td>2009 swine flu pandemic - Wikipedia</td>\n",
       "      <td>The 2009 swine flu pandemic, caused by the H1N...</td>\n",
       "      <td>[https://en.wikiquote.org/wiki/Swine_influenza...</td>\n",
       "      <td>[https://www.nytimes.com/2009/05/28/health/pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANSON-2</td>\n",
       "      <td>http://www.cnn.com/2009/HEALTH/05/28/us.china....</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>China quarantines U.S. school group over flu c...</td>\n",
       "      <td>(CNN)  -- A group of students and teachers fro...</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/2009_H1N1_flu_o...</td>\n",
       "      <td>[http://www.sphere.com/, http://politicalticke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANSON-3</td>\n",
       "      <td>http://hxnxflu.blogspot.com/2009/05/can-scienc...</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>48</td>\n",
       "      <td>H1N1 Flu by crabsallover: Can science save us ...</td>\n",
       "      <td>Read more: Special report on swine fluIS THE w...</td>\n",
       "      <td>[http://hxnxflu.blogspot.com/2009/05/buy-tamif...</td>\n",
       "      <td>[http://www.direct.gov.uk/en/groups/dg_digital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANSON-4</td>\n",
       "      <td>https://www.cdc.gov/h1n1flu/information_h1n1_v...</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>CDC H1N1 Flu | Origin of 2009 H1N1 Flu (Swine ...</td>\n",
       "      <td>Content on this page was developed during the ...</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/2009_H1N1_flu_o...</td>\n",
       "      <td>[https://www.hhs.gov/, https://www.cdc.gov/az/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANSON-5</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/2009_swine_f...</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>2009 swine flu pandemic - Simple English Wikip...</td>\n",
       "      <td>The 2009 flu pandemic or swine flu was an infl...</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/2009_H1N1_flu_o...</td>\n",
       "      <td>[https://fa.wikipedia.org/wiki/%D8%AF%D9%86%DB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DOCNO                                                URL  WAVENO  \\\n",
       "0  ANSON-1  https://en.wikipedia.org/wiki/2009_swine_flu_p...       0   \n",
       "1  ANSON-2  http://www.cnn.com/2009/HEALTH/05/28/us.china....       1   \n",
       "2  ANSON-3  http://hxnxflu.blogspot.com/2009/05/can-scienc...       1   \n",
       "3  ANSON-4  https://www.cdc.gov/h1n1flu/information_h1n1_v...       1   \n",
       "4  ANSON-5  https://simple.wikipedia.org/wiki/2009_swine_f...       1   \n",
       "\n",
       "   OUTLINKNO  INLINK_COUNT                                              TITLE  \\\n",
       "0        602             2                2009 swine flu pandemic - Wikipedia   \n",
       "1          2             2  China quarantines U.S. school group over flu c...   \n",
       "2        139            48  H1N1 Flu by crabsallover: Can science save us ...   \n",
       "3         55             3  CDC H1N1 Flu | Origin of 2009 H1N1 Flu (Swine ...   \n",
       "4         87             2  2009 swine flu pandemic - Simple English Wikip...   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  The 2009 swine flu pandemic, caused by the H1N...   \n",
       "1  (CNN)  -- A group of students and teachers fro...   \n",
       "2  Read more: Special report on swine fluIS THE w...   \n",
       "3  Content on this page was developed during the ...   \n",
       "4  The 2009 flu pandemic or swine flu was an infl...   \n",
       "\n",
       "                                             INLINKS  \\\n",
       "0  [https://en.wikiquote.org/wiki/Swine_influenza...   \n",
       "1  [https://en.wikipedia.org/wiki/2009_H1N1_flu_o...   \n",
       "2  [http://hxnxflu.blogspot.com/2009/05/buy-tamif...   \n",
       "3  [https://en.wikipedia.org/wiki/2009_H1N1_flu_o...   \n",
       "4  [https://en.wikipedia.org/wiki/2009_H1N1_flu_o...   \n",
       "\n",
       "                                            OUTLINKS  \n",
       "0  [https://www.nytimes.com/2009/05/28/health/pol...  \n",
       "1  [http://www.sphere.com/, http://politicalticke...  \n",
       "2  [http://www.direct.gov.uk/en/groups/dg_digital...  \n",
       "3  [https://www.hhs.gov/, https://www.cdc.gov/az/...  \n",
       "4  [https://fa.wikipedia.org/wiki/%D8%AF%D9%86%DB...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the content of the document.txt file\n",
    "with open(document_name, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "with open(inlinks_name, \"r\") as file:\n",
    "    inlinks = json.load(file)\n",
    "\n",
    "# load the outlinks file and replace } with }\\n\n",
    "with open(outlinks_name, 'r') as f:\n",
    "    outlinks = f.read().replace(']}', ']}\\n')\n",
    "outlinks_raw = outlinks.split('\\n')[:-1]\n",
    "\n",
    "outlinks = {}\n",
    "for outlink in outlinks_raw:\n",
    "    outlink = json.loads(outlink)\n",
    "    outlinks.update(outlink)\n",
    "\n",
    "\n",
    "# Split the content into individual documents\n",
    "documents = re.findall(r'<DOC>(.*?)</DOC>', data, re.DOTALL)\n",
    "\n",
    "# Initialize lists to store extracted information\n",
    "docnos = []\n",
    "wavenos = []\n",
    "outlinknos = []\n",
    "texts = []\n",
    "titles = []\n",
    "\n",
    "# Parse each document\n",
    "for doc in documents:\n",
    "    # Extract DOCNO\n",
    "    docno_match = re.search(r'<DOCNO>(.*?)</DOCNO>', doc, re.DOTALL)\n",
    "    if docno_match:\n",
    "        docno = docno_match.group(1)\n",
    "    else:\n",
    "        docno = None\n",
    "    \n",
    "    # Extract WAVENO\n",
    "    waveno_match = re.search(r'<WAVENO>(.*?)</WAVENO>', doc, re.DOTALL)\n",
    "    if waveno_match:\n",
    "        waveno = waveno_match.group(1)\n",
    "    else:\n",
    "        waveno = None\n",
    "    \n",
    "    # Extract OUTLINKNO\n",
    "    outlinkno_match = re.search(r'<OUTLINKNO>(.*?)</OUTLINKNO>', doc, re.DOTALL)\n",
    "    if outlinkno_match:\n",
    "        outlinkno = outlinkno_match.group(1)\n",
    "    else:\n",
    "        outlinkno = None\n",
    "    \n",
    "    # Extract TEXT\n",
    "    text_match = re.search(r'<TEXT>(.*?)</TEXT>', doc, re.DOTALL)\n",
    "    if text_match:\n",
    "        text = text_match.group(1).strip()\n",
    "    else:\n",
    "        text = None\n",
    "    \n",
    "    # Extract TITLE\n",
    "    title_match = re.search(r'<TITLE>(.*?)</TITLE>', doc, re.DOTALL)\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "    else:\n",
    "        title = None\n",
    "    \n",
    "    # Append extracted information to lists\n",
    "    docnos.append(docno)\n",
    "    wavenos.append(waveno)\n",
    "    outlinknos.append(outlinkno)\n",
    "    texts.append(text)\n",
    "    titles.append(title)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"DOCNO\": docnos,\n",
    "    \"WAVENO\": wavenos,\n",
    "    \"OUTLINKNO\": outlinknos,\n",
    "    \"TEXT\": texts,\n",
    "    \"TITLE\": titles\n",
    "})\n",
    "\n",
    "df['URL'] = df['DOCNO'].apply(lambda x: x.split(':',1)[-1])\n",
    "df['DOCNO'] = df['DOCNO'].apply(lambda x: x.split(':',1)[0])\n",
    "\n",
    "inlink_counts = {url : len(inlinks[url]) for url in inlinks.keys()}\n",
    "df['INLINK_COUNT'] = df['URL'].map(inlink_counts)\n",
    "\n",
    "df['INLINKS'] = df['URL'].map(inlinks)\n",
    "df['OUTLINKS'] = df['URL'].map(outlinks)\n",
    "\n",
    "df['WAVENO'] = df['WAVENO'].astype('int')\n",
    "df['OUTLINKNO'] = df['OUTLINKNO'].astype('int')\n",
    "df['INLINK_COUNT'].fillna(0, inplace=True)\n",
    "df['INLINK_COUNT'] = df['INLINK_COUNT'].astype('int')\n",
    "df['OUTLINKS'] = df['OUTLINKS'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df['INLINKS'] = df['INLINKS'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df['TITLE'].fillna(\"\", inplace=True)\n",
    "\n",
    "# arange the columns\n",
    "df = df[['DOCNO', 'URL', 'WAVENO', 'OUTLINKNO', 'INLINK_COUNT', 'TITLE', 'TEXT', 'INLINKS', 'OUTLINKS']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/pnd4sxn93218rhbf3zhwszv00000gn/T/ipykernel_38169/80135727.py:63: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=INDEX_NAME, body = configurations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents\n",
      "Creating Metadata for threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124420/124420 [00:43<00:00, 2855.92it/s] \n",
      "100%|██████████| 124420/124420 [10:00<00:00, 207.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('stoplist.txt', 'r') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\" : stop_words\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"stemmer\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\" : {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            },\n",
    "\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            },\n",
    "            \"author\": {\"type\": \"keyword\"},\n",
    "            \"inlink_no\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"outlink_no\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"inlinks\": {\n",
    "                \"type\": \"keyword\",\n",
    "            },\n",
    "            \"outlinks\": {\n",
    "                \"type\": \"keyword\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# delete the index if it exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "\n",
    "es.indices.create(index=INDEX_NAME, body = configurations)\n",
    "\n",
    "def add_data(id, title, text, inlink_no, outlink_no, inlinks, outlinks):\n",
    "    es.index(index=INDEX_NAME, id=id, document={'author' : ['Anson'], 'title': title, 'content': text, 'inlink_no' : inlink_no, 'outlink_no' : int(outlink_no), 'inlinks' : inlinks, 'outlinks' : outlinks})\n",
    "\n",
    "def add_data_wrapper(row):\n",
    "    add_data(row['URL'], row['TITLE'], row['TEXT'], row['INLINK_COUNT'], row['OUTLINKNO'], row['INLINKS'], row['OUTLINKS'])\n",
    "\n",
    "print(\"Indexing documents\")\n",
    "# Loop through all documents\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    print(\"Creating Metadata for threads\")\n",
    "    futures = [executor.submit(add_data_wrapper, row) for _, row in tqdm(df.iterrows(), total = df.shape[0])]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        pass  # Ensure all tasks are completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
